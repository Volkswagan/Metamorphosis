{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "696c3ea2-2a66-406f-8485-e019dff7cdb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace these variables with your own values\n",
    "gcs_bucket_name = \"<your-gcs-bucket-name>\"\n",
    "mount_name = \"/mnt/<your-mount-name>\"\n",
    "secret_scope = \"gcs-secrets\"\n",
    "secret_key = \"gcp-service-account-key\"\n",
    "\n",
    "# Fetch the service account key from Databricks secrets\n",
    "service_account_key = dbutils.secrets.get(scope=secret_scope, key=secret_key)\n",
    "\n",
    "# Mount the GCS bucket\n",
    "dbutils.fs.mount(\n",
    "  source = f\"gs://{gcs_bucket_name}\",\n",
    "  mount_point = mount_name,\n",
    "  extra_configs = {\n",
    "    \"google.cloud.auth.service.account.json.keyfile\": service_account_key\n",
    "  }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df993f3f-e030-4c62-905c-5d9826c4f3f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps to Implement the Solution:\n",
    "1. Create a Databricks Access Token: You will need an access token to authenticate API requests to Databricks.\n",
    "2. Set Up a Cloud Function: Write a Cloud Function in Python that gets triggered by a GCS event (e.g., file creation).\n",
    "3. Call Databricks API from the Cloud Function: Use the Cloud Function to make an API request to Databricks to run a specific notebook.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "import requests\n",
    "import base64\n",
    "from google.cloud import storage\n",
    "\n",
    "def trigger_databricks_notebook(data, context):\n",
    "    # Define Databricks API parameters\n",
    "    DATABRICKS_INSTANCE = \"https://<your-databricks-instance>\"\n",
    "    DATABRICKS_TOKEN = \"<your-databricks-token>\"\n",
    "    JOB_ID = \"<your-databricks-job-id>\"\n",
    "\n",
    "    # Extract information about the uploaded file from the event data\n",
    "    bucket_name = data['bucket']\n",
    "    file_name = data['name']\n",
    "\n",
    "    print(f\"File {file_name} uploaded to {bucket_name}\")\n",
    "\n",
    "    # Make API request to trigger Databricks notebook job\n",
    "    url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/run-now\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"job_id\": JOB_ID,\n",
    "        \"notebook_params\": {\n",
    "            \"bucket_name\": bucket_name,\n",
    "            \"file_name\": file_name\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Databricks notebook triggered successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to trigger Databricks notebook. Response: {response.text}\")\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "import base64\n",
    "import json\n",
    "import requests\n",
    "from google.cloud import logging\n",
    "from google.cloud import secretmanager\n",
    "\n",
    "# Initialize Cloud Logging client\n",
    "logging_client = logging.Client()\n",
    "logger = logging_client.logger(\"databricks-trigger-logs\")\n",
    "\n",
    "# Function to trigger Databricks job\n",
    "def trigger_databricks_job(event, context):\n",
    "    try:\n",
    "        # Access secrets from Secret Manager\n",
    "        secret_manager_client = secretmanager.SecretManagerServiceClient()\n",
    "        databricks_token_secret = secret_manager_client.access_secret_version(\n",
    "            request={\"name\": \"projects/YOUR_PROJECT_ID/secrets/DATABRICKS_TOKEN/versions/latest\"}\n",
    "        )\n",
    "        databricks_token = databricks_token_secret.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        databricks_instance = \"https://<your-databricks-instance>\"\n",
    "        job_id = \"<your-databricks-job-id>\"\n",
    "        \n",
    "        # Define API URL and headers\n",
    "        url = f\"{databricks_instance}/api/2.1/jobs/run-now\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {databricks_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        # Define payload with dynamic parameters if needed\n",
    "        payload = {\n",
    "            \"job_id\": job_id,\n",
    "            \"notebook_params\": {\n",
    "                \"run_date\": \"2024-09-01\"  # Example dynamic parameter\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Send API request to start the job\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            logger.log_text(\"Databricks job triggered successfully.\")\n",
    "        else:\n",
    "            logger.log_text(f\"Failed to trigger Databricks job: {response.text}\", severity=\"ERROR\")\n",
    "            raise Exception(f\"Databricks job trigger failed with status code {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.log_text(f\"Error in Cloud Function: {str(e)}\", severity=\"ERROR\")\n",
    "        notify_failure_via_email(str(e))  # Example function to send an email on failure\n",
    "\n",
    "def notify_failure_via_email(error_message):\n",
    "    # Use another Cloud Function or any email service to notify on failure\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8da1bb8-a59c-4f1d-9739-be4e947be327",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load new data\n",
    "last_run_time = ... # Retrieve last run timestamp\n",
    "df_incremental = spark.read.format(\"source_format\").load(\"source_path\").filter(col(\"timestamp\") > last_run_time)\n",
    "\n",
    "# Transform data\n",
    "transformed_df = df_incremental.withColumn(\"new_column\", ...)\n",
    "\n",
    "# Upsert data into Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"delta_table_path\")\n",
    "delta_table.alias(\"t\").merge(\n",
    "    transformed_df.alias(\"s\"),\n",
    "    \"t.id = s.id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\n",
    "        \"column1\": \"s.column1\",\n",
    "        \"column2\": \"s.column2\"\n",
    "    }\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Update last processed time\n",
    "new_last_run_time = df_incremental.agg({\"timestamp\": \"max\"}).collect()[0][0]\n",
    "# Save new_last_run_time to metadata store or table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "068f7d6a-a891-4da4-84df-417aa3781499",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"table_wise_emp\") \\\n",
    "    .addColumns(schema) \\\n",
    "    .location(path) \\\n",
    "    .execute()\n",
    "\n",
    "df.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"append\").save(path)\n",
    "\n",
    "rdf = spark.read.format(\"delta\") \\\n",
    "        .option(\"timestampAsOf\", \"2024-07-27T19:37:11.000+00:00\") \\\n",
    "        .option(\"versionAsOf\", 1) \\\n",
    "        .load(path)\n",
    "rdf.show()\n",
    "\n",
    "delta_instance = DeltaTable.forPath(spark, path)\n",
    "delta_instance.delete((col(\"employee_id\") == 4) & (col(\"age\") == 28))\n",
    "\n",
    "delta_instance.update(\n",
    "    condition=(col(\"employee_id\") == 5) & (col(\"first_name\") == \"Mike\"),\n",
    "    set={\"department\": \"'New_Finance'\"}\n",
    ")\n",
    "\n",
    "delta_instance.restoreToTimestamp(\"2024-07-27T19:37:05.000+00:00\")\n",
    "delta_instance.restoreToVersion(1)\n",
    "\n",
    "rdf = delta_instance.toDF()\n",
    "rdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26a5ca30-209b-488d-8301-064e216275b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#SCD Type 1\n",
    "delta_instance = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "delta_instance.alias(\"target\").merge(\n",
    "    updates_df.alias(\"source\"),\n",
    "    \"target.employee_id = source.employee_id\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"department\": col(\"source.department\"),\n",
    "    \"updated_at\": current_timestamp()\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"employee_id\": col(\"source.employee_id\"),\n",
    "    \"department\": col(\"source.department\"),\n",
    "    \"created_at\": current_timestamp(),\n",
    "    \"updated_at\": current_timestamp()\n",
    "}).execute()\n",
    "\n",
    "rdf = delta_instance.toDF()\n",
    "rdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a54eecef-6a94-460f-9146-f1b6b4ce514e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#SCD Type 2\n",
    "joinedDF = sourceDF.join(targetDF, (sourceDF.pk1 == targetDF.pk1) & (sourceDF.pk2 == targetDF.pk2) & (targetDF.active_status == 'Y'), \"left\") \\\n",
    "    .select(sourceDF[\"*\"], targetDF.pk1.alias(\"target_pk1\"), targetDF.pk2.alias(\"target_pk2\"), targetDF.dim1.alias(\"target_dim1\"))\n",
    "filterDF = joinedDF.filter(xxhash64(joinedDF.dim1) != xxhash64(joinedDF.target_dim1))\n",
    "mergeDF = filterDF.withColumn(\"MERGEKEY\", concat(filterDF.pk1, filterDF.pk2))\n",
    "dummyDF = filterDF.filter(\"target_pk1 is not null\").withColumn(\"MERGEKEY\", lit(None))\n",
    "scdDF = mergeDF.union(dummyDF)\n",
    "targetTable.alias(\"target\").merge(\n",
    "    source = scdDF.alias(\"source\"),\n",
    "    condition = \"concat(target.pk1, target.pk2) = source.MERGEKEY and target.active_status = 'Y'\"\n",
    ").whenMatchedUpdate(\n",
    "    set = {\n",
    "        \"active_status\" : \"'N'\",\n",
    "        \"end_date\" : current_date()\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"pk1\" : \"source.pk1\",\n",
    "        \"pk2\" : \"source.pk2\",\n",
    "        \"dim1\" : \"source.dim1\",\n",
    "        \"active_status\" : \"'Y'\",\n",
    "        \"start_date\" : current_date(),\n",
    "        \"end_date\" : \"to_date('9999-12-31', 'yyyy-MM-dd')\"\n",
    "    }\n",
    ").execute()\n",
    "resultDF = targetTable.toDF()\n",
    "resultDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39862d79-cc82-4b8a-8fd1-036112a312b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ALTER TABLE silver_table SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\n",
    "changes_df = spark.read.format(\"delta\").option(\"readChangeData\", True).option(\"startingVersion\", 2).table(\"silver_table\")\n",
    "\n",
    "changes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c56dbce6-c55f-49e6-a875-5317de3eb0db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "window_spec = Window.partitionBy(\"Country\", \"Product\").orderBy(col(\"_commit_version\").desc())\n",
    "df_latest_version = spark.read.format(\"delta\").option(\"versionAsOf\", 5).table(\"silver_table\")\\\n",
    "                            .filter(col(\"_change_type\") != 'update_preimage') \\\n",
    "                            .withColumn(\"rank\", rank().over(window_spec))\\\n",
    "                            .filter(col(\"rank\") == 1)\n",
    "df_latest_version.show()\n",
    "# Create or replace a temporary view with the latest version of the data\n",
    "df_latest_version.createOrReplaceTempView(\"silver_Table_latest_version\")\n",
    "\n",
    "\n",
    "df_gold = spark.read.format(\"delta\").load(gold_table_path)\n",
    "df_silver_latest = spark.table(\"silver_Table_latest_version\")\n",
    "\n",
    "# Define a DataFrame for updates\n",
    "df_updates = df_silver_latest.filter(col(\"_change_type\") == \"update_postimage\") \\\n",
    "            .join(df_gold, [\"Country\", \"Product\"], \"inner\") \\\n",
    "            .select(df_silver_latest[\"Country\"], df_silver_latest[\"Product\"], \n",
    "            (df_silver_latest[\"Sales\"] / df_silver_latest[\"Stock\"]).alias(\"SalesRate\"))\n",
    "\n",
    "df_deletes = df_silver_latest.filter(col(\"_change_type\") == \"delete\") \\\n",
    "    .join(df_gold, [\"Country\", \"Product\"], \"inner\")\n",
    "\n",
    "df_inserts = df_silver_latest.filter(col(\"_change_type\").isNull() | (col(\"_change_type\") == \"insert\")) \\\n",
    "    .select(\"Country\", \"Product\", (col(\"Sales\") / col(\"Stock\")).alias(\"SalesRate\"))\n",
    "\n",
    "df_gold.alias(\"target\").merge(\n",
    "    df_updates.alias(\"source\"),\n",
    "    \"target.Country = source.Country AND target.Product = source.Product\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"SalesRate\": col(\"source.SalesRate\")\n",
    "}).execute()\n",
    "\n",
    "\n",
    "df_gold.alias(\"target\").join(\n",
    "    df_deletes.alias(\"source\"),\n",
    "    [\"Country\", \"Product\"],\n",
    "    \"left_anti\"\n",
    ").write.format(\"delta\").mode(\"overwrite\").save(gold_table_path)\n",
    "\n",
    "df_inserts.write.format(\"delta\").mode(\"append\").save(gold_table_path)\n",
    "\n",
    "# Optionally: Verify the results\n",
    "df_gold.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databrick_Specific",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
