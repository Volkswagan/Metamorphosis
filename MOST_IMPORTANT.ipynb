{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968b8ce4-4da9-4dd2-a478-0a34275c7d30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4fee22-4137-4ef7-858b-0b9b506cca67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://10.172.247.164:40001\n===============================================================================================================\nspark.databricks.preemption.enabled : true\nspark.sql.hive.metastore.jars : /databricks/databricks-hive/*\nspark.driver.tempDirectory : /local_disk0/tmp\nspark.sql.warehouse.dir : dbfs:/user/hive/warehouse\nspark.databricks.managedCatalog.clientClassName : com.databricks.managedcatalog.ManagedCatalogClientImpl\nspark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider\nspark.hadoop.fs.fcfs-s3.impl.disable.cache : true\nspark.sql.streaming.checkpointFileManagerClass : com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager\nspark.databricks.service.dbutils.repl.backend : com.databricks.dbconnect.ReplDBUtils\nspark.hadoop.databricks.s3.verifyBucketExists.enabled : false\nspark.streaming.driver.writeAheadLog.allowBatching : true\nspark.databricks.clusterSource : UI\nspark.hadoop.hive.server2.transport.mode : http\nspark.executor.memory : 8278m\nspark.hadoop.spark.driverproxy.customHeadersToProperties : X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name,X-Databricks-Oauth-Identity-Custom-Claim:spark.databricks.oauthCustomIdentityClaims,X-Databricks-Workload-Id:spark.databricks.workload.id,X-Databricks-Workload-Class:spark.databricks.workload.name\nspark.hadoop.fs.cpfs-adl.impl.disable.cache : true\nspark.databricks.clusterUsageTags.hailEnabled : false\nspark.databricks.clusterUsageTags.clusterLogDeliveryEnabled : false\nspark.databricks.clusterUsageTags.containerType : LXC\nspark.hadoop.fs.s3a.assumed.role.credentials.provider : shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksInstanceProfileCredentialsProvider\nspark.eventLog.enabled : false\nspark.databricks.clusterUsageTags.isIMv2Enabled : false\nspark.hadoop.fs.stage.impl.disable.cache : true\nspark.hadoop.hive.hmshandler.retry.interval : 2000\nspark.executor.tempDirectory : /local_disk0/tmp\nspark.hadoop.fs.azure.authorization.caching.enable : false\nspark.app.id : local-1721498297598\nspark.hadoop.fs.fcfs-abfss.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.clusterUsageTags.orgId : 3349800431166879\nspark.hadoop.mapred.output.committer.class : com.databricks.backend.daemon.data.client.DirectOutputCommitter\nspark.hadoop.hive.server2.thrift.http.port : 10000\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version : 2\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3 : 0\nspark.databricks.clusterUsageTags.driverPublicDns : ec2-52-39-117-195.us-west-2.compute.amazonaws.com\nspark.databricks.eventLog.enabled : true\nspark.sql.allowMultipleContexts : false\nspark.home : /databricks/spark\nspark.databricks.clusterUsageTags.clusterTargetWorkers : 0\nspark.hadoop.hive.server2.idle.operation.timeout : 7200000\nspark.task.reaper.enabled : true\nspark.databricks.clusterUsageTags.autoTerminationMinutes : 60\nspark.storage.memoryFraction : 0.5\nspark.databricks.clusterUsageTags.clusterFirstOnDemand : 0\nspark.databricks.sql.configMapperClass : com.databricks.dbsql.config.SqlConfigMapperBridge\nspark.driver.maxResultSize : 4g\nspark.databricks.clusterUsageTags.sparkEnvVarContainsNewline : false\nspark.hadoop.fs.fcfs-s3.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.delta.multiClusterWrites.enabled : true\nspark.worker.cleanup.enabled : false\nspark.sql.legacy.createHiveTableByDefault : false\nspark.databricks.driver.preferredMavenCentralMirrorUrl : https://maven-central.storage-download.googleapis.com/maven2/\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File : 0\nspark.hadoop.fs.fcfs-s3a.impl.disable.cache : true\nspark.ui.port : 40001\nspark.hadoop.fs.s3a.attempts.maximum : 10\nspark.databricks.clusterUsageTags.enableCredentialPassthrough : false\nspark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign : false\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeType : ebs_volume_type: GENERAL_PURPOSE_SSD\n\nspark.databricks.clusterUsageTags.enableJdbcAutoStart : true\nspark.hadoop.fs.azure.user.agent.prefix : \nspark.hadoop.fs.s3n.impl.disable.cache : true\nspark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough : false\nspark.hadoop.fs.fcfs-s3n.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.hadoop.fs.abfs.impl : com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.s3a.retry.throttle.interval : 500ms\nspark.hadoop.fs.wasb.impl.disable.cache : true\nspark.databricks.clusterUsageTags.clusterLogDestination : \nspark.databricks.wsfsPublicPreview : true\nspark.cleaner.referenceTracking.blocking : false\nspark.databricks.clusterUsageTags.isSingleUserCluster : false\nspark.databricks.clusterUsageTags.clusterState : Pending\nspark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes : false\nspark.databricks.tahoe.logStore.azure.class : com.databricks.tahoe.store.AzureLogStore\nspark.hadoop.fs.azure.skip.metrics : true\nspark.hadoop.hive.hmshandler.retry.attempts : 10\nspark.databricks.clusterUsageTags.driverInstanceId : i-036d51b0621a1bfa6\nspark.scheduler.mode : FAIR\nspark.sql.sources.default : delta\nspark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled : true\nspark.hadoop.fs.cpfs-s3n.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.databricks.clusterUsageTags.clusterWorkers : 0\nspark.hadoop.fs.cpfs-adl.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.databricks.clusterUsageTags.sparkImageLabel : release__12.2.x-snapshot-scala2.12__databricks-universe__12.2.30__58da24d__b0040e8__jenkins__7fbab7c__format-3\nspark.hadoop.fs.fcfs-s3n.impl.disable.cache : true\nspark.hadoop.fs.cpfs-abfss.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.databricks.clusterUsageTags.clusterAllTags : [{\"key\":\"Name\",\"value\":\"ce-worker\"},{\"key\":\"WorkspaceId\",\"value\":\"3349800431166879\"},{\"key\":\"ClusterId\",\"value\":\"0720-175717-jbsjshtk\"}]\nspark.databricks.rocksDB.fileManager.useCommitService : false\nspark.databricks.passthrough.oauth.refresher.impl : com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient\nspark.hadoop.databricks.loki.fileStatusCache.gcs.enabled : false\nspark.sql.hive.metastore.sharedPrefixes : org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\nspark.databricks.io.directoryCommit.enableLogicalDelete : false\nspark.task.reaper.killTimeout : 60s\nspark.repl.class.outputDir : /local_disk0/tmp/repl/spark-5192950247026544858-ec387525-2f64-4757-b435-ad2a2c64aab0\nspark.hadoop.parquet.block.size.row.check.min : 10\nspark.hadoop.hive.server2.use.SSL : true\nspark.databricks.clusterUsageTags.userId : 7928855627714950\nspark.databricks.clusterUsageTags.clusterAvailability : ON_DEMAND\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb : 0\nspark.hadoop.hive.server2.keystore.path : /databricks/keys/jetty-ssl-driver-keystore.jks\nspark.hadoop.fs.gs.impl : com.databricks.common.filesystem.LokiFileSystem\nspark.databricks.credential.redactor : com.databricks.logging.secrets.CredentialRedactorProxyImpl\nspark.databricks.clusterUsageTags.clusterPinned : false\nspark.databricks.acl.provider : com.databricks.sql.acl.ReflectionBackedAclProvider\nspark.databricks.mlflow.autologging.enabled : true\nspark.extraListeners : com.databricks.backend.daemon.driver.DBCEventLoggingListener\nspark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled : false\nspark.sql.parquet.cacheMetadata : true\nspark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2 : 0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss : 0\nspark.hadoop.parquet.abfs.readahead.optimization.enabled : true\nspark.hadoop.fs.adl.impl : com.databricks.adl.AdlFileSystem\nspark.hadoop.fs.cpfs-abfss.impl.disable.cache : true\nspark.hadoop.fs.abfss.impl : com.databricks.common.filesystem.LokiFileSystem\nspark.databricks.clusterUsageTags.enableLocalDiskEncryption : false\nspark.databricks.tahoe.logStore.class : com.databricks.tahoe.store.DelegatingLogStore\nspark.hadoop.fs.s3.impl.disable.cache : true\nspark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins : 30\nspark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins : 30\nlibraryDownload.sleepIntervalSeconds : 5\nspark.databricks.cloudProvider : AWS\nspark.sql.hive.convertMetastoreParquet : true\nspark.executor.id : driver\nspark.databricks.service.dbutils.server.backend : com.databricks.dbconnect.SparkServerDBUtils\nspark.databricks.clusterUsageTags.workerEnvironmentId : default-worker-env\nspark.databricks.repl.enableClassFileCleanup : true\nspark.databricks.clusterUsageTags.clusterName : My Cluster\nspark.hadoop.fs.s3a.multipart.size : 10485760\nspark.databricks.clusterUsageTags.cloudProvider : AWS\nspark.metrics.conf : /databricks/spark/conf/metrics.properties\nspark.akka.frameSize : 256\nspark.hadoop.fs.s3a.fast.upload : true\nspark.hadoop.fs.wasbs.impl : shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\nspark.sql.streaming.stopTimeout : 15s\nspark.hadoop.hive.server2.keystore.password : [REDACTED]\nspark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting : false\nspark.hadoop.fs.s3a.retry.interval : 250ms\nspark.databricks.clusterUsageTags.sparkEnvVarContainsEscape : false\nspark.databricks.overrideDefaultCommitProtocol : org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\nspark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass : com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient\nspark.databricks.clusterUsageTags.clusterNoDriverDaemon : false\nlibraryDownload.timeoutSeconds : 180\nspark.hadoop.parquet.memory.pool.ratio : 0.5\nspark.databricks.clusterUsageTags.shardName : devtierprod1\nspark.databricks.clusterUsageTags.clusterScalingType : fixed_size\nspark.hadoop.databricks.loki.fileStatusCache.abfs.enabled : false\nspark.databricks.passthrough.adls.gen2.tokenProviderClassName : com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider\nspark.databricks.clusterUsageTags.driverInstancePrivateIp : 10.172.241.104\nspark.hadoop.fs.s3a.block.size : 67108864\nspark.repl.class.uri : spark://10.172.247.164:43375/classes\nspark.databricks.tahoe.logStore.gcp.class : com.databricks.tahoe.store.GCPLogStore\nspark.serializer.objectStreamReset : 100\nspark.databricks.clusterUsageTags.sparkMasterUrlType : None\nspark.sql.sources.commitProtocolClass : com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs : 0\nspark.databricks.clusterUsageTags.effectiveSparkVersion : 12.2.x-scala2.12\nspark.databricks.clusterUsageTags.attribute_tag_budget : \nspark.hadoop.fs.fcfs-s3a.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.clusterUsageTags.currentAttemptContainerZoneId : us-west-2c\nspark.databricks.clusterUsageTags.clusterPythonVersion : 3\nspark.databricks.clusterUsageTags.enableDfAcls : false\nspark.driver.host : 10.172.247.164\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount : 0\nspark.hadoop.databricks.loki.fileSystemCache.enabled : true\nspark.shuffle.service.enabled : true\nspark.hadoop.fs.file.impl : com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem\nspark.plugins : org.apache.spark.sql.connect.SparkConnectPlugin\nspark.hadoop.fs.fcfs-wasb.impl.disable.cache : true\nspark.hadoop.fs.cpfs-s3.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.databricks.clusterUsageTags.containerZoneId : auto\nspark.databricks.clusterUsageTags.attribute_tag_dust_maintainer : \nspark.hadoop.fs.s3a.multipart.threshold : 104857600\nspark.rpc.message.maxSize : 256\nspark.databricks.clusterUsageTags.attribute_tag_dust_suite : \nspark.hadoop.fs.fcfs-wasbs.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.driverNfs.enabled : true\nspark.databricks.clusterUsageTags.clusterMetastoreAccessType : RDS_DIRECT\nspark.databricks.clusterUsageTags.ngrokNpipEnabled : false\nspark.hadoop.parquet.page.metadata.validation.enabled : true\nspark.databricks.clusterUsageTags.instanceProfileUsed : false\nspark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName : com.databricks.unity.TokenServiceApiTokenProvider\nspark.databricks.passthrough.glue.executorServiceFactoryClassName : com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory\nspark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus : false\nspark.databricks.acl.scim.client : com.databricks.spark.sql.acl.client.DriverToWebappScimClient\nspark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick : false\nspark.hadoop.fs.adl.impl.disable.cache : true\nspark.hadoop.parquet.block.size.row.check.max : 10\nspark.hadoop.fs.s3a.connection.maximum : 200\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2 : 0\nspark.hadoop.fs.s3a.fast.upload.active.blocks : 32\nspark.shuffle.reduceLocality.enabled : false\nspark.databricks.clusterUsageTags.driverNodeType : dev-tier-node\nspark.hadoop.spark.sql.sources.outputCommitterClass : com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter\nspark.hadoop.fs.fcfs-abfs.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.clusterUsageTags.instanceBootstrapType : ssh\nspark.hadoop.fs.fcfs-abfss.impl.disable.cache : true\nspark.hadoop.hive.server2.thrift.http.cookie.auth.enabled : false\nspark.hadoop.spark.hadoop.aws.glue.cache.table.size : 1000\nspark.databricks.driverNodeTypeId : dev-tier-node\nspark.sql.parquet.compression.codec : snappy\nspark.hadoop.fs.stage.impl : com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem\nspark.databricks.credential.scope.fs.s3a.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider\nspark.databricks.cloudfetch.hasRegionSupport : true\nspark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories : false\nspark.hadoop.fs.wasb.impl : shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\nspark.r.sql.derby.temp.dir : /tmp/RtmppGiaLV\nspark.hadoop.spark.hadoop.aws.glue.cache.db.size : 1000\nspark.databricks.workerNodeTypeId : dev-tier-node\nspark.databricks.passthrough.glue.credentialsProviderFactoryClassName : com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory\nspark.databricks.clusterUsageTags.clusterEbsVolumeSize : 0\nspark.sparklyr-backend.threads : 1\nspark.databricks.clusterUsageTags.clusterId : 0720-175717-jbsjshtk\nspark.hadoop.fs.fcfs-wasb.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.passthrough.s3a.tokenProviderClassName : com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider\nspark.databricks.session.share : false\nspark.databricks.clusterUsageTags.clusterResourceClass : default\nspark.driver.port : 43375\nspark.databricks.isShieldWorkspace : false\nspark.hadoop.fs.idbfs.impl : com.databricks.io.idbfs.IdbfsFileSystem\nspark.driver.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\nspark.databricks.cloudfetch.requestDownloadUrlsWithHeaders : false\nspark.databricks.telemetry.prometheus.samplingRate : 100\nspark.hadoop.fs.dbfs.impl : com.databricks.backend.daemon.data.client.DbfsHadoop3\nspark.databricks.clusterUsageTags.clusterSku : STANDARD_SKU\nspark.hadoop.fs.gs.impl.disable.cache : true\nspark.databricks.privateLinkEnabled : false\nspark.databricks.clusterUsageTags.clusterUnityCatalogMode : CUSTOM\nspark.delta.sharing.profile.provider.class : io.delta.sharing.DeltaSharingCredentialsProvider\nspark.executor.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1\nspark.databricks.clusterUsageTags.isGroupCluster : false\nspark.app.startTime : 1721498289655\nspark.worker.aioaLazyConfig.iamReadinessCheckClientClass : com.databricks.backend.daemon.driver.NephosIamRoleCheckClient\nspark.databricks.clusterUsageTags.clusterEbsVolumeType : GENERAL_PURPOSE_SSD\nspark.databricks.clusterUsageTags.clusterOwnerOrgId : 3349800431166879\nspark.databricks.automl.serviceEnabled : true\nspark.hadoop.parquet.page.size.check.estimate : false\nspark.databricks.clusterUsageTags.attribute_tag_service : \nspark.databricks.passthrough.s3a.threadPoolExecutor.factory.class : com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory\nspark.databricks.clusterUsageTags.driverContainerId : 64f692c93e67419eba1ab4477df8f7b8\nspark.databricks.metrics.filesystem_io_metrics : true\nspark.databricks.cloudfetch.requesterClassName : com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester\nspark.master : local[8]\nspark.databricks.delta.logStore.crossCloud.fatal : true\nspark.databricks.driverNfs.clusterWidePythonLibsEnabled : true\nspark.files.fetchFailure.unRegisterOutputOnHost : true\nspark.databricks.clusterUsageTags.enableSqlAclsOnly : false\nspark.databricks.clusterUsageTags.clusterEbsVolumeCount : 0\nspark.databricks.clusterUsageTags.clusterSizeType : VM_CONTAINER\nspark.hadoop.databricks.fs.perfMetrics.enable : true\nspark.databricks.clusterUsageTags.clusterNumSshKeys : 0\nspark.hadoop.fs.gs.outputstream.upload.chunk.size : 16777216\nspark.databricks.tahoe.logStore.aws.class : com.databricks.tahoe.store.S3LockBasedLogStore\nspark.speculation.quantile : 0.9\nspark.databricks.clusterUsageTags.privateLinkEnabled : false\nspark.shuffle.manager : SORT\nspark.files.overwrite : true\nspark.databricks.credential.aws.secretKey.redactor : com.databricks.spark.util.AWSSecretKeyRedactorProxy\nspark.databricks.clusterUsageTags.clusterNumCustomTags : 0\nspark.hadoop.fs.s3.impl : com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.s3a.impl.disable.cache : true\nspark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes : false\nspark.r.numRBackendThreads : 1\nspark.hadoop.fs.wasbs.impl.disable.cache : true\nspark.hadoop.fs.abfss.impl.disable.cache : true\nspark.hadoop.fs.azure.cache.invalidator.type : com.databricks.encryption.utils.CacheInvalidatorImpl\nspark.sql.hive.metastore.version : 0.13.0\nspark.shuffle.service.port : 4048\nspark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType : default\nspark.databricks.sparkContextId : 5192950247026544858\nspark.databricks.acl.client : com.databricks.spark.sql.acl.client.SparkSqlAclClient\nspark.streaming.driver.writeAheadLog.closeFileAfterWrite : true\nspark.hadoop.hive.warehouse.subdir.inherit.perms : false\nspark.databricks.clusterUsageTags.driverContainerPrivateIp : 10.172.247.164\nspark.databricks.clusterUsageTags.runtimeEngine : STANDARD\nspark.databricks.clusterUsageTags.isServicePrincipalCluster : false\nspark.databricks.credential.scope.fs.impl : com.databricks.sql.acl.fs.CredentialScopeFileSystem\nspark.hadoop.databricks.loki.fileStatusCache.s3a.enabled : false\nspark.databricks.enablePublicDbfsFuse : false\nspark.databricks.clusterUsageTags.enableElasticDisk : false\nspark.hadoop.fs.fcfs-wasbs.impl.disable.cache : true\nspark.databricks.clusterUsageTags.userProvidedSparkVersion : 12.2.x-scala2.12\nspark.databricks.clusterUsageTags.clusterNodeType : dev-tier-node\nspark.databricks.passthrough.adls.tokenProviderClassName : com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider\nspark.app.name : Databricks Shell\nspark.driver.allowMultipleContexts : false\nspark.hadoop.fs.AbstractFileSystem.gs.impl : shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\nspark.databricks.secret.sparkConf.keys.toRedact : \nspark.rdd.compress : true\nspark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException : false\nspark.databricks.python.defaultPythonRepl : ipykernel\nspark.hadoop.fs.s3a.retry.limit : 6\nspark.databricks.clusterUsageTags.attribute_tag_dust_execution_env : \nspark.databricks.eventLog.dir : eventlogs\nspark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider\nspark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled : false\nspark.databricks.driverNfs.pathSuffix : .ephemeral_nfs\nspark.databricks.clusterUsageTags.clusterCreator : Webapp\nspark.speculation : false\nspark.hadoop.databricks.dbfs.client.version : v1\nspark.hadoop.hive.server2.session.check.interval : 60000\nspark.sql.hive.convertCTAS : true\nspark.hadoop.fs.s3a.max.total.tasks : 1000\nspark.hadoop.spark.sql.parquet.output.committer.class : org.apache.spark.sql.parquet.DirectParquetOutputCommitter\nspark.databricks.clusterUsageTags.sparkVersion : 12.2.x-scala2.12\nspark.hadoop.fs.s3a.fast.upload.default : true\nspark.databricks.clusterUsageTags.clusterGeneration : 0\nspark.hadoop.fs.mlflowdbfs.impl : com.databricks.mlflowdbfs.MlflowdbfsFileSystem\nspark.databricks.eventLog.listenerClassName : com.databricks.backend.daemon.driver.DBCEventLoggingListener\nspark.hadoop.fs.abfs.impl.disable.cache : true\nspark.speculation.multiplier : 3\nspark.storage.blockManagerTimeoutIntervalMs : 300000\nspark.databricks.clusterUsageTags.instanceWorkerEnvId : default-worker-env\nspark.sparkr.use.daemon : false\nspark.scheduler.listenerbus.eventqueue.capacity : 20000\nspark.hadoop.fs.s3a.impl : com.databricks.common.filesystem.LokiFileSystem\nspark.databricks.clusterUsageTags.clusterStateMessage : Starting Spark\nspark.hadoop.parquet.page.write-checksum.enabled : true\nspark.hadoop.databricks.s3commit.client.sslTrustAll : false\nspark.hadoop.fs.s3a.threads.max : 136\nspark.r.backendConnectionTimeout : 604800\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs : 0\nspark.hadoop.fs.s3n.impl : com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.hive.server2.idle.session.timeout : 900000\nspark.databricks.redactor : com.databricks.spark.util.DatabricksSparkLogRedactorProxy\nspark.executor.extraClassPath : /databricks/spark/dbconf/log4j/executor:/\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n.maxAttempts: 4\nspark.databricks.delta.optimize.incremental: true\nspark.databricks.delta.optimize.lineage: true\nspark.databricks.delta.optimizeWrite.enabled: <undefined>\nspark.databricks.delta.prepareDeltaScan.parallel.enabled: true\nspark.databricks.delta.prepareDeltaScan.threadpool.size: 64\nspark.databricks.delta.properties.defaults.minReaderVersion: 1\nspark.databricks.delta.properties.defaults.minWriterVersion: 2\nspark.databricks.delta.replaceWhere.constraintCheck.enabled: true\nspark.databricks.delta.replaceWhere.dataColumns.enabled: true\nspark.databricks.delta.restore.protocolDowngradeAllowed: false\nspark.databricks.delta.retentionDurationCheck.enabled: true\nspark.databricks.delta.schema.autoMerge.enabled: false\nspark.databricks.delta.stalenessLimit: 0ms\nspark.databricks.delta.vacuum.logging.enabled: <undefined>\nspark.databricks.delta.vacuum.parallelDelete.enabled: false\nspark.databricks.delta.vacuum.parallelDelete.parallelism: <undefined>\nspark.databricks.delta.withEventTimeOrder.enabled: <undefined>\nspark.databricks.delta.write.copyParquetFiles.enabled: false\nspark.databricks.delta.writeChecksumFile.enabled: true\nspark.databricks.execution.arrowCollect.maxBytesPerBatch: 1048576\nspark.databricks.execution.arrowCollect.maxRecordsPerBatch: 2500\nspark.databricks.execution.bloomFilterSaturationThreshold: 0.8\nspark.databricks.hive.metastore.client.pool.enabled: true\nspark.databricks.hive.metastore.client.pool.log.fullStackTrace: true\nspark.databricks.hive.metastore.client.pool.minimumIdle: 0\nspark.databricks.hive.metastore.client.pool.size: 20\nspark.databricks.hive.metastore.client.pool.type: <undefined>\nspark.databricks.hive.metastore.client.pool.waitTime: -1\nspark.databricks.hive.metastore.connection.maxIdleMillis: 60000\nspark.databricks.io.cache.parquet.enabled: true\nspark.databricks.io.cache.parquet.numWriters: 1\nspark.databricks.io.cache.prefix: dbfs,s3,hdfs,wasb,adl,abfs,gs,cpfs-,mcfs-,delta-sharing\nspark.databricks.io.cache.unified.enabled: true\nspark.databricks.io.hive.convertMetastoreAvro: false\nspark.databricks.io.parquet.verifyChecksumOnWrite.schemes: s3,s3a,abfs,abfss,dbfs\nspark.databricks.managedCatalog.gcs.tokenProviderClassName: \nspark.databricks.optimizer.OptimizeArrayNullCheck: true\nspark.databricks.optimizer.OptimizeArraySize: true\nspark.databricks.optimizer.adaptive.exchange.targetPartitionSize: 67108864b\nspark.databricks.optimizer.adaptive.excludedRules: <undefined>\nspark.databricks.optimizer.dynamicFilterPropagation.enabled: true\nspark.databricks.optimizer.dynamicPartitionPruning: true\nspark.databricks.optimizer.eliminateNullCheckForExtractValue: true\nspark.databricks.optimizer.multiGenerateNestedSchemaPruning.enabled: true\nspark.databricks.optimizer.nestedColumnPruningForDatasetTo: true\nspark.databricks.optimizer.rangeJoin.binSize: 0.0\nspark.databricks.photon.bloomFilterPassthroughRowCount: 409600\nspark.databricks.photon.bloomFilterRateMinimumRowCount: 40960\nspark.databricks.photon.bloomFilterRateThreshold: 0.8\nspark.databricks.photon.bloomFilterSaturationCheckInterval: 10\nspark.databricks.photon.shuffleRuntimeBloomFilterMaxNumBits: 67108864\nspark.databricks.photon.shuffleRuntimeBloomFilterNumBits: 8388608\nspark.databricks.photon.shuffleRuntimeBloomFilterNumBitsPerRow: 8\nspark.databricks.photon.shuffleRuntimeBloomFilterNumPartitions: 10\nspark.databricks.photon.shuffleToBloomSizeRatio: 10\nspark.databricks.pyspark.pythonUdfsOnly: false\nspark.databricks.service.client.session.cache.size: 20\nspark.databricks.service.dbutils.fs.parallel.ls.threadPoolSize: 20\nspark.databricks.service.dbutils.fs.parallel.ls.timeoutSeconds: 7200\nspark.databricks.sql.externalUDF.perBatchTimeoutSeconds: 300\nspark.databricks.sql.externalUDF.sessionCreationTimeoutSeconds: 300\nspark.databricks.sql.files.prorateMaxPartitionBytes.ubound: 1073741824b\nspark.databricks.sql.functions.readFiles.optimizeLimit: true\nspark.databricks.sql.initial.catalog.name: hive_metastore\nspark.databricks.sql.maxTaskSizeForBucketedJoin: 268435456b\nspark.databricks.sql.minBucketsForBucketedJoin: <undefined>\nspark.databricks.sql.optimizer.maxAliasReplacementProjectListSize: 100\nspark.databricks.sql.optimizer.maxAliasReplacementProjectedExpressionsListSize: 500\nspark.databricks.streaming.schemaEvolution.enabled: true\nspark.databricks.thriftserver.getTables.threadPool.size: 64\nspark.databricks.thriftserver.metadata.timeout.seconds: 0ms\nspark.databricks.thriftserver.metadataTable.legacyMatch.enabled: false\nspark.databricks.unityCatalog.allowCrossMetastoreViews: false\nspark.databricks.unityCatalog.clientSideDualCatalog.enabled: true\nspark.databricks.unityCatalog.enabled: false\nspark.databricks.unityCatalog.thriftGetColumnsOptimization: true\nspark.databricks.unityCatalog.thriftGetColumnsOptimizationIgnoreUnnecessary: true\nspark.databricks.unityCatalog.validateDependencies: true\nspark.sql.adaptive.advisoryPartitionSizeInBytes: 67108864b\nspark.sql.adaptive.autoBroadcastJoinThreshold: <undefined>\nspark.sql.adaptive.coalescePartitions.enabled: true\nspark.sql.adaptive.coalescePartitions.initialPartitionNum: <undefined>\nspark.sql.adaptive.coalescePartitions.minPartitionSize: 1MB\nspark.sql.adaptive.coalescePartitions.parallelismFirst: true\nspark.sql.adaptive.customCostEvaluatorClass: <undefined>\nspark.sql.adaptive.enabled: true\nspark.sql.adaptive.forceOptimizeSkewedJoin: false\nspark.sql.adaptive.localShuffleReader.enabled: true\nspark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold: 0b\nspark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled: true\nspark.sql.adaptive.optimizer.excludedRules: <undefined>\nspark.sql.adaptive.rebalancePartitionsSmallPartitionFactor: 0.2\nspark.sql.adaptive.skewJoin.enabled: true\nspark.sql.adaptive.skewJoin.skewedPartitionFactor: 5.0\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes: 256MB\nspark.sql.ansi.doubleQuotedIdentifiers: false\nspark.sql.ansi.enabled: false\nspark.sql.ansi.enforceAnsiTypeCoercion: true\nspark.sql.ansi.enforceReservedKeywords: false\nspark.sql.ansi.relationPrecedence: false\nspark.sql.autoBroadcastJoinThreshold: 10MB\nspark.sql.avro.compression.codec: snappy\nspark.sql.avro.deflate.level: -1\nspark.sql.avro.filterPushdown.enabled: true\nspark.sql.broadcastTimeout: -1000ms\nspark.sql.bucketing.coalesceBucketsInJoin.enabled: false\nspark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio: 4\nspark.sql.cache.serializer: org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer\nspark.sql.catalog.spark_catalog: <undefined>\nspark.sql.catalog.spark_catalog.defaultDatabase: default\nspark.sql.cbo.enabled: true\nspark.sql.cbo.joinReorder.dp.star.filter: false\nspark.sql.cbo.joinReorder.dp.threshold: 12\nspark.sql.cbo.joinReorder.enabled: true\nspark.sql.cbo.planStats.enabled: false\nspark.sql.cbo.starSchemaDetection: false\nspark.sql.charAsVarchar: false\nspark.sql.cli.print.header: false\nspark.sql.columnNameOfCorruptRecord: _corrupt_record\nspark.sql.csv.filterPushdown.enabled: true\nspark.sql.datetime.java8API.enabled: false\nspark.sql.debug.maxToStringFields: 25\nspark.sql.defaultCatalog: spark_catalog\nspark.sql.error.messageFormat: PRETTY\nspark.sql.event.truncate.length: 2147483647\nspark.sql.execution.arrow.enabled: false\nspark.sql.execution.arrow.fallback.enabled: true\nspark.sql.execution.arrow.localRelationThreshold: 48MB\nspark.sql.execution.arrow.maxRecordsPerBatch: 10000\nspark.sql.execution.arrow.pyspark.enabled: false\nspark.sql.execution.arrow.pyspark.fallback.enabled: true\nspark.sql.execution.arrow.pyspark.selfDestruct.enabled: false\nspark.sql.execution.arrow.sparkr.enabled: false\nspark.sql.execution.pandas.udf.buffer.size: 65536\nspark.sql.execution.pyspark.udf.simplifiedTraceback.enabled: true\nspark.sql.execution.topKSortFallbackThreshold: 2147483632\nspark.sql.extensions: <undefined>\nspark.sql.files.ignoreCorruptFiles: false\nspark.sql.files.ignoreMissingFiles: false\nspark.sql.files.maxPartitionBytes: 128MB\nspark.sql.files.maxRecordsPerFile: 0\nspark.sql.files.minPartitionNum: <undefined>\nspark.sql.function.concatBinaryAsString: false\nspark.sql.function.eltOutputAsString: false\nspark.sql.groupByAliases: true\nspark.sql.groupByOrdinal: true\nspark.sql.hive.convertInsertingPartitionedTable: true\nspark.sql.hive.convertMetastoreCtas: true\nspark.sql.hive.convertMetastoreInsertDir: true\nspark.sql.hive.convertMetastoreOrc: true\nspark.sql.hive.convertMetastoreParquet: true\nspark.sql.hive.convertMetastoreParquet.mergeSchema: false\nspark.sql.hive.filesourcePartitionFileCacheSize: 262144000\nspark.sql.hive.manageFilesourcePartitions: true\nspark.sql.hive.metastore.barrierPrefixes: \nspark.sql.hive.metastore.jars: /databricks/databricks-hive/*\nspark.sql.hive.metastore.jars.path: \nspark.sql.hive.metastore.sharedPrefixes: org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\nspark.sql.hive.metastore.version: 0.13.0\nspark.sql.hive.metastorePartitionPruning: true\nspark.sql.hive.metastorePartitionPruningFallbackOnException: false\nspark.sql.hive.metastorePartitionPruningFastFallback: false\nspark.sql.hive.thriftServer.async: true\nspark.sql.hive.thriftServer.singleSession: false\nspark.sql.hive.verifyPartitionPath: false\nspark.sql.hive.version: 2.3.9\nspark.sql.inMemoryColumnarStorage.batchSize: 10000\nspark.sql.inMemoryColumnarStorage.compressed: true\nspark.sql.inMemoryColumnarStorage.enableVectorizedReader: true\nspark.sql.json.filterPushdown.enabled: true\nspark.sql.jsonGenerator.ignoreNullFields: true\nspark.sql.leafNodeDefaultParallelism: <undefined>\nspark.sql.mapKeyDedupPolicy: EXCEPTION\nspark.sql.maven.additionalRemoteRepositories: https://maven-central.storage-download.googleapis.com/maven2/\nspark.sql.maxMetadataStringLength: 100\nspark.sql.maxPlanStringLength: 2147483632\nspark.sql.maxSinglePartitionBytes: 9223372036854775807b\nspark.sql.metadataCacheTTLSeconds: -1000ms\nspark.sql.optimizer.collapseProjectAlwaysInline: false\nspark.sql.optimizer.dynamicPartitionPruning.enabled: true\nspark.sql.optimizer.enableCsvExpressionOptimization: true\nspark.sql.optimizer.enableJsonExpressionOptimization: true\nspark.sql.optimizer.excludedRules: <undefined>\nspark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold: 10GB\nspark.sql.optimizer.runtime.bloomFilter.creationSideThreshold: 10MB\nspark.sql.optimizer.runtime.bloomFilter.enabled: true\nspark.sql.optimizer.runtime.bloomFilter.expectedNumItems: 1000000\nspark.sql.optimizer.runtime.bloomFilter.maxNumBits: 67108864\nspark.sql.optimizer.runtime.bloomFilter.maxNumItems: 4000000\nspark.sql.optimizer.runtime.bloomFilter.numBits: 8388608\nspark.sql.optimizer.runtime.rowLevelOperationGroupFilter.enabled: true\nspark.sql.optimizer.runtimeFilter.number.threshold: 10\nspark.sql.orc.aggregatePushdown: false\nspark.sql.orc.columnarReaderBatchSize: 4096\nspark.sql.orc.compression.codec: snappy\nspark.sql.orc.enableNestedColumnVectorizedReader: true\nspark.sql.orc.enableVectorizedReader: true\nspark.sql.orc.filterPushdown: true\nspark.sql.orc.mergeSchema: false\nspark.sql.orderByOrdinal: true\nspark.sql.parquet.aggregatePushdown: false\nspark.sql.parquet.binaryAsString: false\nspark.sql.parquet.columnarReaderBatchSize: 4096\nspark.sql.parquet.compression.codec: snappy\nspark.sql.parquet.enableNestedColumnVectorizedReader: false\nspark.sql.parquet.enableVectorizedReader: true\nspark.sql.parquet.fieldId.read.enabled: true\nspark.sql.parquet.fieldId.read.ignoreMissing: false\nspark.sql.parquet.fieldId.write.enabled: true\nspark.sql.parquet.filterPushdown: true\nspark.sql.parquet.int96AsTimestamp: true\nspark.sql.parquet.int96TimestampConversion: false\nspark.sql.parquet.mergeSchema: false\nspark.sql.parquet.outputTimestampType: INT96\nspark.sql.parquet.recordLevelFilter.enabled: false\nspark.sql.parquet.respectSummaryFiles: false\nspark.sql.parquet.writeLegacyFormat: false\nspark.sql.parser.quotedRegexColumnNames: false\nspark.sql.pivotMaxValues: 10000\nspark.sql.pyspark.inferNestedDictAsStruct.enabled: false\nspark.sql.pyspark.jvmStacktrace.enabled: false\nspark.sql.queryExecutionListeners: <undefined>\nspark.sql.readSideCharPadding: false\nspark.sql.redaction.options.regex: (?i)url\nspark.sql.redaction.string.regex: <undefined>\nspark.sql.repl.eagerEval.enabled: false\nspark.sql.repl.eagerEval.maxNumRows: 20\nspark.sql.repl.eagerEval.truncate: 20\nspark.sql.session.timeZone: Etc/UTC\nspark.sql.shuffle.partitions: 200\nspark.sql.shuffledHashJoinFactor: 3\nspark.sql.sources.bucketing.autoBucketedScan.enabled: true\nspark.sql.sources.bucketing.enabled: true\nspark.sql.sources.bucketing.maxBuckets: 100000\nspark.sql.sources.default: delta\nspark.sql.sources.disabledJdbcConnProviderList: \nspark.sql.sources.parallelPartitionDiscovery.threshold: 32\nspark.sql.sources.partitionColumnTypeInference.enabled: true\nspark.sql.sources.partitionOverwriteMode: STATIC\nspark.sql.sources.v2.bucketing.enabled: false\nspark.sql.sources.v2.bucketing.pushPartValues.enabled: false\nspark.sql.statistics.fallBackToHdfs: false\nspark.sql.statistics.histogram.enabled: false\nspark.sql.statistics.size.autoUpdate.enabled: false\nspark.sql.storeAssignmentPolicy: ANSI\nspark.sql.streaming.checkpointLocation: <undefined>\nspark.sql.streaming.continuous.epochBacklogQueueSize: 10000\nspark.sql.streaming.disabledV2Writers: \nspark.sql.streaming.fileSource.cleaner.numThreads: 1\nspark.sql.streaming.forceDeleteTempCheckpointLocation: false\nspark.sql.streaming.metricsEnabled: false\nspark.sql.streaming.multipleWatermarkPolicy: min\nspark.sql.streaming.noDataMicroBatches.enabled: true\nspark.sql.streaming.numRecentProgressUpdates: 100\nspark.sql.streaming.sessionWindow.merge.sessions.in.local.partition: false\nspark.sql.streaming.stateStore.stateSchemaCheck: true\nspark.sql.streaming.stopActiveRunOnRestart: true\nspark.sql.streaming.stopTimeout: 15s\nspark.sql.streaming.streamingQueryListeners: <undefined>\nspark.sql.streaming.ui.enabled: true\nspark.sql.streaming.ui.retainedProgressUpdates: 100\nspark.sql.streaming.ui.retainedQueries: 100\nspark.sql.thriftServer.interruptOnCancel: false\nspark.sql.thriftServer.queryTimeout: 0ms\nspark.sql.thriftserver.scheduler.pool: <undefined>\nspark.sql.thriftserver.ui.retainedSessions: 200\nspark.sql.thriftserver.ui.retainedStatements: 200\nspark.sql.ui.explainMode: formatted\nspark.sql.ui.retainedExecutions: 1000\nspark.sql.variable.substitute: true\nspark.sql.warehouse.dir: dbfs:/user/hive/warehouse\nspark.thriftserver.arrowBasedRowSet.maxBytesPerFetchLimit: 10485760\nspark.thriftserver.arrowBasedRowSet.timestampAsString: true\nspark.thriftserver.cloudStoreBasedRowSet.driver.cloudUploadThreshold: 5242880b\nspark.thriftserver.cloudStoreBasedRowSet.enabled: true\nspark.thriftserver.cloudStoreBasedRowSet.executor.cloudUploadThreshold: 5242880b\nspark.thriftserver.cloudStoreBasedRowSet.maxBytesPerFetchLimit: 1073741824\nspark.thriftserver.cloudStoreBasedRowSet.resultFilesInMetadataLimit: 10000\nspark.thriftserver.cloudStoreBasedRowSet.smallResultsOptimization.enabled: true\nspark.thriftserver.cloudfetch.enabled: true\nspark.thriftserver.intervalsAsStringInResultSchema: true\nspark.thriftserver.operation.poll.timeout: 600000ms\nspark.thriftserver.operation.poll.timeout.enabled: true\nspark.thriftserver.resultCaching.eagerLookup.enabled: false\n===============================================================================================================\nspark.sql.adaptive.advisoryPartitionSizeInBytes: 67108864b\nspark.sql.adaptive.autoBroadcastJoinThreshold: <undefined>\nspark.sql.adaptive.coalescePartitions.enabled: true\nspark.sql.adaptive.coalescePartitions.initialPartitionNum: <undefined>\nspark.sql.adaptive.coalescePartitions.minPartitionSize: 1MB\nspark.sql.adaptive.coalescePartitions.parallelismFirst: true\nspark.sql.adaptive.customCostEvaluatorClass: <undefined>\nspark.sql.adaptive.enabled: true\nspark.sql.adaptive.forceOptimizeSkewedJoin: false\nspark.sql.adaptive.localShuffleReader.enabled: true\nspark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold: 0b\nspark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled: true\nspark.sql.adaptive.optimizer.excludedRules: <undefined>\nspark.sql.adaptive.rebalancePartitionsSmallPartitionFactor: 0.2\nspark.sql.adaptive.skewJoin.enabled: true\nspark.sql.adaptive.skewJoin.skewedPartitionFactor: 5.0\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes: 256MB\nspark.sql.ansi.doubleQuotedIdentifiers: false\nspark.sql.ansi.enabled: false\nspark.sql.ansi.enforceAnsiTypeCoercion: true\nspark.sql.ansi.enforceReservedKeywords: false\nspark.sql.ansi.relationPrecedence: false\nspark.sql.autoBroadcastJoinThreshold: 10MB\nspark.sql.avro.compression.codec: snappy\nspark.sql.avro.deflate.level: -1\nspark.sql.avro.filterPushdown.enabled: true\nspark.sql.broadcastTimeout: -1000ms\nspark.sql.bucketing.coalesceBucketsInJoin.enabled: false\nspark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio: 4\nspark.sql.cache.serializer: org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer\nspark.sql.catalog.spark_catalog: <undefined>\nspark.sql.catalog.spark_catalog.defaultDatabase: default\nspark.sql.cbo.enabled: true\nspark.sql.cbo.joinReorder.dp.star.filter: false\nspark.sql.cbo.joinReorder.dp.threshold: 12\nspark.sql.cbo.joinReorder.enabled: true\nspark.sql.cbo.planStats.enabled: false\nspark.sql.cbo.starSchemaDetection: false\nspark.sql.charAsVarchar: false\nspark.sql.cli.print.header: false\nspark.sql.columnNameOfCorruptRecord: _corrupt_record\nspark.sql.csv.filterPushdown.enabled: true\nspark.sql.datetime.java8API.enabled: false\nspark.sql.debug.maxToStringFields: 25\nspark.sql.defaultCatalog: spark_catalog\nspark.sql.error.messageFormat: PRETTY\nspark.sql.event.truncate.length: 2147483647\nspark.sql.execution.arrow.enabled: false\nspark.sql.execution.arrow.fallback.enabled: true\nspark.sql.execution.arrow.localRelationThreshold: 48MB\nspark.sql.execution.arrow.maxRecordsPerBatch: 10000\nspark.sql.execution.arrow.pyspark.enabled: false\nspark.sql.execution.arrow.pyspark.fallback.enabled: true\nspark.sql.execution.arrow.pyspark.selfDestruct.enabled: false\nspark.sql.execution.arrow.sparkr.enabled: false\nspark.sql.execution.pandas.udf.buffer.size: 65536\nspark.sql.execution.pyspark.udf.simplifiedTraceback.enabled: true\nspark.sql.execution.topKSortFallbackThreshold: 2147483632\nspark.sql.extensions: <undefined>\nspark.sql.files.ignoreCorruptFiles: false\nspark.sql.files.ignoreMissingFiles: false\nspark.sql.files.maxPartitionBytes: 128MB\nspark.sql.files.maxRecordsPerFile: 0\nspark.sql.files.minPartitionNum: <undefined>\nspark.sql.function.concatBinaryAsString: false\nspark.sql.function.eltOutputAsString: false\nspark.sql.groupByAliases: true\nspark.sql.groupByOrdinal: true\nspark.sql.hive.convertInsertingPartitionedTable: true\nspark.sql.hive.convertMetastoreCtas: true\nspark.sql.hive.convertMetastoreInsertDir: true\nspark.sql.hive.convertMetastoreOrc: true\nspark.sql.hive.convertMetastoreParquet: true\nspark.sql.hive.convertMetastoreParquet.mergeSchema: false\nspark.sql.hive.filesourcePartitionFileCacheSize: 262144000\nspark.sql.hive.manageFilesourcePartitions: true\nspark.sql.hive.metastore.barrierPrefixes: \nspark.sql.hive.metastore.jars: /databricks/databricks-hive/*\nspark.sql.hive.metastore.jars.path: \nspark.sql.hive.metastore.sharedPrefixes: org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\nspark.sql.hive.metastore.version: 0.13.0\nspark.sql.hive.metastorePartitionPruning: true\nspark.sql.hive.metastorePartitionPruningFallbackOnException: false\nspark.sql.hive.metastorePartitionPruningFastFallback: false\nspark.sql.hive.thriftServer.async: true\nspark.sql.hive.thriftServer.singleSession: false\nspark.sql.hive.verifyPartitionPath: false\nspark.sql.hive.version: 2.3.9\nspark.sql.inMemoryColumnarStorage.batchSize: 10000\nspark.sql.inMemoryColumnarStorage.compressed: true\nspark.sql.inMemoryColumnarStorage.enableVectorizedReader: true\nspark.sql.json.filterPushdown.enabled: true\nspark.sql.jsonGenerator.ignoreNullFields: true\nspark.sql.leafNodeDefaultParallelism: <undefined>\nspark.sql.mapKeyDedupPolicy: EXCEPTION\nspark.sql.maven.additionalRemoteRepositories: https://maven-central.storage-download.googleapis.com/maven2/\nspark.sql.maxMetadataStringLength: 100\nspark.sql.maxPlanStringLength: 2147483632\nspark.sql.maxSinglePartitionBytes: 9223372036854775807b\nspark.sql.metadataCacheTTLSeconds: -1000ms\nspark.sql.optimizer.collapseProjectAlwaysInline: false\nspark.sql.optimizer.dynamicPartitionPruning.enabled: true\nspark.sql.optimizer.enableCsvExpressionOptimization: true\nspark.sql.optimizer.enableJsonExpressionOptimization: true\nspark.sql.optimizer.excludedRules: <undefined>\nspark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold: 10GB\nspark.sql.optimizer.runtime.bloomFilter.creationSideThreshold: 10MB\nspark.sql.optimizer.runtime.bloomFilter.enabled: true\nspark.sql.optimizer.runtime.bloomFilter.expectedNumItems: 1000000\nspark.sql.optimizer.runtime.bloomFilter.maxNumBits: 67108864\nspark.sql.optimizer.runtime.bloomFilter.maxNumItems: 4000000\nspark.sql.optimizer.runtime.bloomFilter.numBits: 8388608\nspark.sql.optimizer.runtime.rowLevelOperationGroupFilter.enabled: true\nspark.sql.optimizer.runtimeFilter.number.threshold: 10\nspark.sql.orc.aggregatePushdown: false\nspark.sql.orc.columnarReaderBatchSize: 4096\nspark.sql.orc.compression.codec: snappy\nspark.sql.orc.enableNestedColumnVectorizedReader: true\nspark.sql.orc.enableVectorizedReader: true\nspark.sql.orc.filterPushdown: true\nspark.sql.orc.mergeSchema: false\nspark.sql.orderByOrdinal: true\nspark.sql.parquet.aggregatePushdown: false\nspark.sql.parquet.binaryAsString: false\nspark.sql.parquet.columnarReaderBatchSize: 4096\nspark.sql.parquet.compression.codec: snappy\nspark.sql.parquet.enableNestedColumnVectorizedReader: false\nspark.sql.parquet.enableVectorizedReader: true\nspark.sql.parquet.fieldId.read.enabled: true\nspark.sql.parquet.fieldId.read.ignoreMissing: false\nspark.sql.parquet.fieldId.write.enabled: true\nspark.sql.parquet.filterPushdown: true\nspark.sql.parquet.int96AsTimestamp: true\nspark.sql.parquet.int96TimestampConversion: false\nspark.sql.parquet.mergeSchema: false\nspark.sql.parquet.outputTimestampType: INT96\nspark.sql.parquet.recordLevelFilter.enabled: false\nspark.sql.parquet.respectSummaryFiles: false\nspark.sql.parquet.writeLegacyFormat: false\nspark.sql.parser.quotedRegexColumnNames: false\nspark.sql.pivotMaxValues: 10000\nspark.sql.pyspark.inferNestedDictAsStruct.enabled: false\nspark.sql.pyspark.jvmStacktrace.enabled: false\nspark.sql.queryExecutionListeners: <undefined>\nspark.sql.readSideCharPadding: false\nspark.sql.redaction.options.regex: (?i)url\nspark.sql.redaction.string.regex: <undefined>\nspark.sql.repl.eagerEval.enabled: false\nspark.sql.repl.eagerEval.maxNumRows: 20\nspark.sql.repl.eagerEval.truncate: 20\nspark.sql.session.timeZone: Etc/UTC\nspark.sql.shuffle.partitions: 200\nspark.sql.shuffledHashJoinFactor: 3\nspark.sql.sources.bucketing.autoBucketedScan.enabled: true\nspark.sql.sources.bucketing.enabled: true\nspark.sql.sources.bucketing.maxBuckets: 100000\nspark.sql.sources.default: delta\nspark.sql.sources.disabledJdbcConnProviderList: \nspark.sql.sources.parallelPartitionDiscovery.threshold: 32\nspark.sql.sources.partitionColumnTypeInference.enabled: true\nspark.sql.sources.partitionOverwriteMode: STATIC\nspark.sql.sources.v2.bucketing.enabled: false\nspark.sql.sources.v2.bucketing.pushPartValues.enabled: false\nspark.sql.statistics.fallBackToHdfs: false\nspark.sql.statistics.histogram.enabled: false\nspark.sql.statistics.size.autoUpdate.enabled: false\nspark.sql.storeAssignmentPolicy: ANSI\nspark.sql.streaming.checkpointLocation: <undefined>\nspark.sql.streaming.continuous.epochBacklogQueueSize: 10000\nspark.sql.streaming.disabledV2Writers: \nspark.sql.streaming.fileSource.cleaner.numThreads: 1\nspark.sql.streaming.forceDeleteTempCheckpointLocation: false\nspark.sql.streaming.metricsEnabled: false\nspark.sql.streaming.multipleWatermarkPolicy: min\nspark.sql.streaming.noDataMicroBatches.enabled: true\nspark.sql.streaming.numRecentProgressUpdates: 100\nspark.sql.streaming.sessionWindow.merge.sessions.in.local.partition: false\nspark.sql.streaming.stateStore.stateSchemaCheck: true\nspark.sql.streaming.stopActiveRunOnRestart: true\nspark.sql.streaming.stopTimeout: 15s\nspark.sql.streaming.streamingQueryListeners: <undefined>\nspark.sql.streaming.ui.enabled: true\nspark.sql.streaming.ui.retainedProgressUpdates: 100\nspark.sql.streaming.ui.retainedQueries: 100\nspark.sql.thriftServer.interruptOnCancel: false\nspark.sql.thriftServer.queryTimeout: 0ms\nspark.sql.thriftserver.scheduler.pool: <undefined>\nspark.sql.thriftserver.ui.retainedSessions: 200\nspark.sql.thriftserver.ui.retainedStatements: 200\nspark.sql.ui.explainMode: formatted\nspark.sql.ui.retainedExecutions: 1000\nspark.sql.variable.substitute: true\nspark.sql.warehouse.dir: dbfs:/user/hive/warehouse\nOut[21]: '\\n# Access the SparkContext\\nsc = spark.sparkContext\\nall_props = sc._conf.getAll()\\n\\n# Print the properties\\nfor key, value in all_props:\\n    print(f\"{key}: {value}\")\\n'"
     ]
    }
   ],
   "source": [
    "# cluster details\n",
    "print(spark.sparkContext.uiWebUrl)\n",
    "print(\"===============================================================================================================\")\n",
    "# cluster configuration details\n",
    "for k,v in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{k} : {v}\")\n",
    "print(\"===============================================================================================================\")\n",
    "sql_configs = spark.sql(\"SET -v\").collect()\n",
    "for row in sql_configs:\n",
    "    print(f\"{row['key']}: {row['value']}\")\n",
    "print(\"===============================================================================================================\")\n",
    "# Fetch all Spark SQL configuration properties\n",
    "sql_configs = spark.sql(\"SET -v\").collect()\n",
    "# Filter and print the properties related to Spark SQL\n",
    "for row in sql_configs:\n",
    "    if row['key'].startswith(\"spark.sql\"):\n",
    "        print(f\"{row['key']}: {row['value']}\")\n",
    "'''\n",
    "# Access the SparkContext\n",
    "sc = spark.sparkContext\n",
    "all_props = sc._conf.getAll()\n",
    "\n",
    "# Print the properties\n",
    "for key, value in all_props:\n",
    "    print(f\"{key}: {value}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfb5c918-6c43-4de6-8de6-fab266039353",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.adaptive.advisoryPartitionSizeInBytes: 67108864b\nspark.sql.adaptive.autoBroadcastJoinThreshold: <undefined>\nspark.sql.adaptive.coalescePartitions.enabled: true\nspark.sql.adaptive.coalescePartitions.initialPartitionNum: <undefined>\nspark.sql.adaptive.coalescePartitions.minPartitionSize: 1MB\nspark.sql.adaptive.coalescePartitions.parallelismFirst: true\nspark.sql.adaptive.customCostEvaluatorClass: <undefined>\nspark.sql.adaptive.enabled: true\nspark.sql.adaptive.forceOptimizeSkewedJoin: false\nspark.sql.adaptive.localShuffleReader.enabled: true\nspark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold: 0b\nspark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled: true\nspark.sql.adaptive.optimizer.excludedRules: <undefined>\nspark.sql.adaptive.rebalancePartitionsSmallPartitionFactor: 0.2\nspark.sql.adaptive.skewJoin.enabled: true\nspark.sql.adaptive.skewJoin.skewedPartitionFactor: 5.0\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes: 256MB\nspark.sql.ansi.doubleQuotedIdentifiers: false\nspark.sql.ansi.enabled: false\nspark.sql.ansi.enforceAnsiTypeCoercion: true\nspark.sql.ansi.enforceReservedKeywords: false\nspark.sql.ansi.relationPrecedence: false\nspark.sql.autoBroadcastJoinThreshold: 10MB\nspark.sql.avro.compression.codec: snappy\nspark.sql.avro.deflate.level: -1\nspark.sql.avro.filterPushdown.enabled: true\nspark.sql.broadcastTimeout: -1000ms\nspark.sql.bucketing.coalesceBucketsInJoin.enabled: false\nspark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio: 4\nspark.sql.cache.serializer: org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer\nspark.sql.catalog.spark_catalog: <undefined>\nspark.sql.catalog.spark_catalog.defaultDatabase: default\nspark.sql.cbo.enabled: true\nspark.sql.cbo.joinReorder.dp.star.filter: false\nspark.sql.cbo.joinReorder.dp.threshold: 12\nspark.sql.cbo.joinReorder.enabled: true\nspark.sql.cbo.planStats.enabled: false\nspark.sql.cbo.starSchemaDetection: false\nspark.sql.charAsVarchar: false\nspark.sql.cli.print.header: false\nspark.sql.columnNameOfCorruptRecord: _corrupt_record\nspark.sql.csv.filterPushdown.enabled: true\nspark.sql.datetime.java8API.enabled: false\nspark.sql.debug.maxToStringFields: 25\nspark.sql.defaultCatalog: spark_catalog\nspark.sql.error.messageFormat: PRETTY\nspark.sql.event.truncate.length: 2147483647\nspark.sql.execution.arrow.enabled: false\nspark.sql.execution.arrow.fallback.enabled: true\nspark.sql.execution.arrow.localRelationThreshold: 48MB\nspark.sql.execution.arrow.maxRecordsPerBatch: 10000\nspark.sql.execution.arrow.pyspark.enabled: false\nspark.sql.execution.arrow.pyspark.fallback.enabled: true\nspark.sql.execution.arrow.pyspark.selfDestruct.enabled: false\nspark.sql.execution.arrow.sparkr.enabled: false\nspark.sql.execution.pandas.udf.buffer.size: 65536\nspark.sql.execution.pyspark.udf.simplifiedTraceback.enabled: true\nspark.sql.execution.topKSortFallbackThreshold: 2147483632\nspark.sql.extensions: <undefined>\nspark.sql.files.ignoreCorruptFiles: false\nspark.sql.files.ignoreMissingFiles: false\nspark.sql.files.maxPartitionBytes: 128MB\nspark.sql.files.maxRecordsPerFile: 0\nspark.sql.files.minPartitionNum: <undefined>\nspark.sql.function.concatBinaryAsString: false\nspark.sql.function.eltOutputAsString: false\nspark.sql.groupByAliases: true\nspark.sql.groupByOrdinal: true\nspark.sql.hive.convertInsertingPartitionedTable: true\nspark.sql.hive.convertMetastoreCtas: true\nspark.sql.hive.convertMetastoreInsertDir: true\nspark.sql.hive.convertMetastoreOrc: true\nspark.sql.hive.convertMetastoreParquet: true\nspark.sql.hive.convertMetastoreParquet.mergeSchema: false\nspark.sql.hive.filesourcePartitionFileCacheSize: 262144000\nspark.sql.hive.manageFilesourcePartitions: true\nspark.sql.hive.metastore.barrierPrefixes: \nspark.sql.hive.metastore.jars: /databricks/databricks-hive/*\nspark.sql.hive.metastore.jars.path: \nspark.sql.hive.metastore.sharedPrefixes: org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\nspark.sql.hive.metastore.version: 0.13.0\nspark.sql.hive.metastorePartitionPruning: true\nspark.sql.hive.metastorePartitionPruningFallbackOnException: false\nspark.sql.hive.metastorePartitionPruningFastFallback: false\nspark.sql.hive.thriftServer.async: true\nspark.sql.hive.thriftServer.singleSession: false\nspark.sql.hive.verifyPartitionPath: false\nspark.sql.hive.version: 2.3.9\nspark.sql.inMemoryColumnarStorage.batchSize: 10000\nspark.sql.inMemoryColumnarStorage.compressed: true\nspark.sql.inMemoryColumnarStorage.enableVectorizedReader: true\nspark.sql.json.filterPushdown.enabled: true\nspark.sql.jsonGenerator.ignoreNullFields: true\nspark.sql.leafNodeDefaultParallelism: <undefined>\nspark.sql.mapKeyDedupPolicy: EXCEPTION\nspark.sql.maven.additionalRemoteRepositories: https://maven-central.storage-download.googleapis.com/maven2/\nspark.sql.maxMetadataStringLength: 100\nspark.sql.maxPlanStringLength: 2147483632\nspark.sql.maxSinglePartitionBytes: 9223372036854775807b\nspark.sql.metadataCacheTTLSeconds: -1000ms\nspark.sql.optimizer.collapseProjectAlwaysInline: false\nspark.sql.optimizer.dynamicPartitionPruning.enabled: true\nspark.sql.optimizer.enableCsvExpressionOptimization: true\nspark.sql.optimizer.enableJsonExpressionOptimization: true\nspark.sql.optimizer.excludedRules: <undefined>\nspark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold: 10GB\nspark.sql.optimizer.runtime.bloomFilter.creationSideThreshold: 10MB\nspark.sql.optimizer.runtime.bloomFilter.enabled: true\nspark.sql.optimizer.runtime.bloomFilter.expectedNumItems: 1000000\nspark.sql.optimizer.runtime.bloomFilter.maxNumBits: 67108864\nspark.sql.optimizer.runtime.bloomFilter.maxNumItems: 4000000\nspark.sql.optimizer.runtime.bloomFilter.numBits: 8388608\nspark.sql.optimizer.runtime.rowLevelOperationGroupFilter.enabled: true\nspark.sql.optimizer.runtimeFilter.number.threshold: 10\nspark.sql.orc.aggregatePushdown: false\nspark.sql.orc.columnarReaderBatchSize: 4096\nspark.sql.orc.compression.codec: snappy\nspark.sql.orc.enableNestedColumnVectorizedReader: true\nspark.sql.orc.enableVectorizedReader: true\nspark.sql.orc.filterPushdown: true\nspark.sql.orc.mergeSchema: false\nspark.sql.orderByOrdinal: true\nspark.sql.parquet.aggregatePushdown: false\nspark.sql.parquet.binaryAsString: false\nspark.sql.parquet.columnarReaderBatchSize: 4096\nspark.sql.parquet.compression.codec: snappy\nspark.sql.parquet.enableNestedColumnVectorizedReader: false\nspark.sql.parquet.enableVectorizedReader: true\nspark.sql.parquet.fieldId.read.enabled: true\nspark.sql.parquet.fieldId.read.ignoreMissing: false\nspark.sql.parquet.fieldId.write.enabled: true\nspark.sql.parquet.filterPushdown: true\nspark.sql.parquet.int96AsTimestamp: true\nspark.sql.parquet.int96TimestampConversion: false\nspark.sql.parquet.mergeSchema: false\nspark.sql.parquet.outputTimestampType: INT96\nspark.sql.parquet.recordLevelFilter.enabled: false\nspark.sql.parquet.respectSummaryFiles: false\nspark.sql.parquet.writeLegacyFormat: false\nspark.sql.parser.quotedRegexColumnNames: false\nspark.sql.pivotMaxValues: 10000\nspark.sql.pyspark.inferNestedDictAsStruct.enabled: false\nspark.sql.pyspark.jvmStacktrace.enabled: false\nspark.sql.queryExecutionListeners: <undefined>\nspark.sql.readSideCharPadding: false\nspark.sql.redaction.options.regex: (?i)url\nspark.sql.redaction.string.regex: <undefined>\nspark.sql.repl.eagerEval.enabled: false\nspark.sql.repl.eagerEval.maxNumRows: 20\nspark.sql.repl.eagerEval.truncate: 20\nspark.sql.session.timeZone: Etc/UTC\nspark.sql.shuffle.partitions: 200\nspark.sql.shuffledHashJoinFactor: 3\nspark.sql.sources.bucketing.autoBucketedScan.enabled: true\nspark.sql.sources.bucketing.enabled: true\nspark.sql.sources.bucketing.maxBuckets: 100000\nspark.sql.sources.default: delta\nspark.sql.sources.disabledJdbcConnProviderList: \nspark.sql.sources.parallelPartitionDiscovery.threshold: 32\nspark.sql.sources.partitionColumnTypeInference.enabled: true\nspark.sql.sources.partitionOverwriteMode: STATIC\nspark.sql.sources.v2.bucketing.enabled: false\nspark.sql.sources.v2.bucketing.pushPartValues.enabled: false\nspark.sql.statistics.fallBackToHdfs: false\nspark.sql.statistics.histogram.enabled: false\nspark.sql.statistics.size.autoUpdate.enabled: false\nspark.sql.storeAssignmentPolicy: ANSI\nspark.sql.streaming.checkpointLocation: <undefined>\nspark.sql.streaming.continuous.epochBacklogQueueSize: 10000\nspark.sql.streaming.disabledV2Writers: \nspark.sql.streaming.fileSource.cleaner.numThreads: 1\nspark.sql.streaming.forceDeleteTempCheckpointLocation: false\nspark.sql.streaming.metricsEnabled: false\nspark.sql.streaming.multipleWatermarkPolicy: min\nspark.sql.streaming.noDataMicroBatches.enabled: true\nspark.sql.streaming.numRecentProgressUpdates: 100\nspark.sql.streaming.sessionWindow.merge.sessions.in.local.partition: false\nspark.sql.streaming.stateStore.stateSchemaCheck: true\nspark.sql.streaming.stopActiveRunOnRestart: true\nspark.sql.streaming.stopTimeout: 15s\nspark.sql.streaming.streamingQueryListeners: <undefined>\nspark.sql.streaming.ui.enabled: true\nspark.sql.streaming.ui.retainedProgressUpdates: 100\nspark.sql.streaming.ui.retainedQueries: 100\nspark.sql.thriftServer.interruptOnCancel: false\nspark.sql.thriftServer.queryTimeout: 0ms\nspark.sql.thriftserver.scheduler.pool: <undefined>\nspark.sql.thriftserver.ui.retainedSessions: 200\nspark.sql.thriftserver.ui.retainedStatements: 200\nspark.sql.ui.explainMode: formatted\nspark.sql.ui.retainedExecutions: 1000\nspark.sql.variable.substitute: true\nspark.sql.warehouse.dir: dbfs:/user/hive/warehouse\n"
     ]
    }
   ],
   "source": [
    "# Fetch all Spark SQL configuration properties\n",
    "sql_configs = spark.sql(\"SET -v\").collect()\n",
    "\n",
    "# Filter and print the properties related to Spark SQL\n",
    "for row in sql_configs:\n",
    "    if row['key'].startswith(\"spark.sql\"):\n",
    "        print(f\"{row['key']}: {row['value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da3763b0-1674-46d0-be6f-f7b8aff747f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.hive.metastore.jars: /databricks/databricks-hive/*\nspark.sql.warehouse.dir: dbfs:/user/hive/warehouse\nspark.sql.streaming.checkpointFileManagerClass: com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager\nspark.sql.allowMultipleContexts: false\nspark.sql.legacy.createHiveTableByDefault: false\nspark.sql.sources.default: delta\nspark.sql.hive.metastore.sharedPrefixes: org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\nspark.sql.parquet.cacheMetadata: true\nspark.sql.hive.convertMetastoreParquet: true\nspark.sql.streaming.stopTimeout: 15s\nspark.sql.sources.commitProtocolClass: com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol\nspark.sql.parquet.compression.codec: snappy\nspark.sql.hive.metastore.version: 0.13.0\nspark.sql.hive.convertCTAS: true\n"
     ]
    }
   ],
   "source": [
    "# Fetch all Spark configuration properties\n",
    "spark_conf = spark.sparkContext.getConf().getAll()\n",
    "\n",
    "# Filter properties related to Spark SQL optimizer\n",
    "optimizer_configs = {k: v for k, v in spark_conf if k.startswith(\"spark.sql\")}\n",
    "\n",
    "# Print the optimizer configurations\n",
    "for key, value in optimizer_configs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1855eb5-4b3f-4350-ab9e-66d775595222",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.adaptive.enabled: True\nspark.sql.adaptive.join.enabled: True\nspark.sql.adaptive.skewJoin.enabled: True\nspark.sql.autoBroadcastJoinThreshold: True\nspark.sql.shuffle.partitions: True\n"
     ]
    }
   ],
   "source": [
    "# List of specific Spark SQL optimizer properties to check\n",
    "properties_to_check = [\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.join.enabled\",\n",
    "    \"spark.sql.adaptive.skewJoin.enabled\",\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\",\n",
    "    \"spark.sql.shuffle.partitions\"\n",
    "]\n",
    "\n",
    "# Fetch and print the values of the specific properties\n",
    "for prop in properties_to_check:\n",
    "    print(f\"{prop}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b572330-1f0d-437e-8354-b7703b8c3697",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "com.databricks.sql.io.caching.consistentFileSplitting: true\nspark.databricks.adaptive.autoBroadcastJoinThreshold: 31457280b\nspark.databricks.adaptive.autoOptimizeShuffle.aggregateRatio: 0.1\nspark.databricks.adaptive.autoOptimizeShuffle.aggregateSpillFactor: 0.1\nspark.databricks.adaptive.autoOptimizeShuffle.equalityFilterSelectivity: 0.1\nspark.databricks.adaptive.autoOptimizeShuffle.exchangeSpillFactor: 0.1\nspark.databricks.adaptive.autoOptimizeShuffle.filterSelectivity: 0.5\nspark.databricks.adaptive.autoOptimizeShuffle.joinFactor: 1.0\nspark.databricks.adaptive.autoOptimizeShuffle.joinSpillFactor: 1.0\nspark.databricks.adaptive.autoOptimizeShuffle.maxPartitionNumber: 20480\nspark.databricks.adaptive.autoOptimizeShuffle.minFilterRatio: 0.01\nspark.databricks.adaptive.autoOptimizeShuffle.minPartitionNumber: <undefined>\nspark.databricks.adaptive.autoOptimizeShuffle.preshufflePartitionSizeInBytes: 128MB\nspark.databricks.adaptive.autoOptimizeShuffle.useFileSize: true\nspark.databricks.adaptive.skewJoin.spillProofPartitionThresholdInBytes: 1GB\nspark.databricks.adaptive.skewJoin.spillProofTargetPartitionSizeInBytes: 512MB\nspark.databricks.aether.connectTimeoutMillis: 120000\nspark.databricks.aether.heartbeatPeriodMillis: 30000\nspark.databricks.aether.parallelListFiles.threadPoolSize: 32\nspark.databricks.aether.pipelinedRetryInitialBackoffMillis: 500\nspark.databricks.aether.shuffleFetcher.maxPrefetchDepth: 128\nspark.databricks.aether.shuffleFetcher.threadPoolSize: 128\nspark.databricks.analyzer.tableResolution.metastore.threadpoolSize: 20\nspark.databricks.analyzer.tableResolution.threadpoolSize: 64\nspark.databricks.autotune.maintenance.client.buildupPushIntervalSeconds: 0ms\nspark.databricks.autotune.maintenance.client.enableDebugLogging: false\nspark.databricks.autotune.maintenance.client.maxNumPartitionInfo: 2000\nspark.databricks.autotune.maintenance.client.pushEntriesThreshold: 100\nspark.databricks.autotune.maintenance.client.pushIntervalMinutes: 60000ms\nspark.databricks.autotune.maintenance.client.requestTimeoutSeconds: 55000ms\nspark.databricks.autotune.maintenance.client.retryTimeoutSeconds: 300000ms\nspark.databricks.autotune.maintenance.client.smallScanThresholdBytes: 33554432\nspark.databricks.autotune.maintenance.client.tokenRefreshMinutes: 60000ms\nspark.databricks.autotune.maintenance.client.useUCToken: true\nspark.databricks.autotune.maintenance.client.vacuumTimeoutIntervalMinutes: 0ms\nspark.databricks.catalog.getTablesByName.parallel.enabled: true\nspark.databricks.cloudFiles.allowPrefixChange: true\nspark.databricks.cloudFiles.checkSourceChanged: true\nspark.databricks.cloudFiles.missingMetadataFile.writeNew: false\nspark.databricks.cloudFiles.protocolVersion: <undefined>\nspark.databricks.cloudFiles.rootSchemaLocation: <undefined>\nspark.databricks.cloudFiles.schemaInference.enabled: true\nspark.databricks.cloudFiles.sourceMetrics.enabled: true\nspark.databricks.cloudFiles.sqlApi.enabled: true\nspark.databricks.credentials.assumed.role: <undefined>\nspark.databricks.delta.alterLocation.bypassSchemaCheck: false\nspark.databricks.delta.alterTable.rename.enabledOnAWS: false\nspark.databricks.delta.autoCompact.enabled: <undefined>\nspark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled: false\nspark.databricks.delta.checkLatestSchemaOnRead: true\nspark.databricks.delta.checkpoint.async.enabled: <undefined>\nspark.databricks.delta.checkpoint.triggers.deltaFileThreshold: 8388608b\nspark.databricks.delta.commitInfo.enabled: true\nspark.databricks.delta.commitInfo.userMetadata: <undefined>\nspark.databricks.delta.constraints.assumesDropIfExists.enabled: false\nspark.databricks.delta.convert.iceberg.partitionEvolution.enabled: false\nspark.databricks.delta.convert.iceberg.useNativePartitionValues: true\nspark.databricks.delta.convert.metadataCheck.enabled: true\nspark.databricks.delta.convert.partitionValues.ignoreCastFailure: false\nspark.databricks.delta.convert.useCatalogSchema: true\nspark.databricks.delta.convert.useMetadataLog: true\nspark.databricks.delta.fastQueryPath.dataskipping.checkpointRequired: false\nspark.databricks.delta.fastQueryPath.dataskipping.deltaSizeThreshold: 10485760b\nspark.databricks.delta.fastQueryPath.dataskipping.driverCache.enabled: true\nspark.databricks.delta.fastQueryPath.dataskipping.enabled: true\nspark.databricks.delta.fastQueryPath.enabled: true\nspark.databricks.delta.formatCheck.cache.enabled: true\nspark.databricks.delta.formatCheck.enabled: true\nspark.databricks.delta.history.metricsEnabled: true\nspark.databricks.delta.lastCommitVersionInSession: <undefined>\nspark.databricks.delta.logCleanup.async.disabled: false\nspark.databricks.delta.merge.materializeSource.maxAttempts: 4\nspark.databricks.delta.optimize.incremental: true\nspark.databricks.delta.optimize.lineage: true\nspark.databricks.delta.optimizeWrite.enabled: <undefined>\nspark.databricks.delta.prepareDeltaScan.parallel.enabled: true\nspark.databricks.delta.prepareDeltaScan.threadpool.size: 64\nspark.databricks.delta.properties.defaults.minReaderVersion: 1\nspark.databricks.delta.properties.defaults.minWriterVersion: 2\nspark.databricks.delta.replaceWhere.constraintCheck.enabled: true\nspark.databricks.delta.replaceWhere.dataColumns.enabled: true\nspark.databricks.delta.restore.protocolDowngradeAllowed: false\nspark.databricks.delta.retentionDurationCheck.enabled: true\nspark.databricks.delta.schema.autoMerge.enabled: false\nspark.databricks.delta.stalenessLimit: 0ms\nspark.databricks.delta.vacuum.logging.enabled: <undefined>\nspark.databricks.delta.vacuum.parallelDelete.enabled: false\nspark.databricks.delta.vacuum.parallelDelete.parallelism: <undefined>\nspark.databricks.delta.withEventTimeOrder.enabled: <undefined>\nspark.databricks.delta.write.copyParquetFiles.enabled: false\nspark.databricks.delta.writeChecksumFile.enabled: true\nspark.databricks.execution.arrowCollect.maxBytesPerBatch: 1048576\nspark.databricks.execution.arrowCollect.maxRecordsPerBatch: 2500\nspark.databricks.execution.bloomFilterSaturationThreshold: 0.8\nspark.databricks.hive.metastore.client.pool.enabled: true\nspark.databricks.hive.metastore.client.pool.log.fullStackTrace: true\nspark.databricks.hive.metastore.client.pool.minimumIdle: 0\nspark.databricks.hive.metastore.client.pool.size: 20\nspark.databricks.hive.metastore.client.pool.type: <undefined>\nspark.databricks.hive.metastore.client.pool.waitTime: -1\nspark.databricks.hive.metastore.connection.maxIdleMillis: 60000\nspark.databricks.io.cache.parquet.enabled: true\nspark.databricks.io.cache.parquet.numWriters: 1\nspark.databricks.io.cache.prefix: dbfs,s3,hdfs,wasb,adl,abfs,gs,cpfs-,mcfs-,delta-sharing\nspark.databricks.io.cache.unified.enabled: true\nspark.databricks.io.hive.convertMetastoreAvro: false\nspark.databricks.io.parquet.verifyChecksumOnWrite.schemes: s3,s3a,abfs,abfss,dbfs\nspark.databricks.managedCatalog.gcs.tokenProviderClassName: \nspark.databricks.optimizer.OptimizeArrayNullCheck: true\nspark.databricks.optimizer.OptimizeArraySize: true\nspark.databricks.optimizer.adaptive.exchange.targetPartitionSize: 67108864b\nspark.databricks.optimizer.adaptive.excludedRules: <undefined>\nspark.databricks.optimizer.dynamicFilterPropagation.enabled: true\nspark.databricks.optimizer.dynamicPartitionPruning: true\nspark.databricks.optimizer.eliminateNullCheckForExtractValue: true\nspark.databricks.optimizer.multiGenerateNestedSchemaPruning.enabled: true\nspark.databricks.optimizer.nestedColumnPruningForDatasetTo: true\nspark.databricks.optimizer.rangeJoin.binSize: 0.0\nspark.databricks.photon.bloomFilterPassthroughRowCount: 409600\nspark.databricks.photon.bloomFilterRateMinimumRowCount: 40960\nspark.databricks.photon.bloomFilterRateThreshold: 0.8\nspark.databricks.photon.bloomFilterSaturationCheckInterval: 10\nspark.databricks.photon.shuffleRuntimeBloomFilterMaxNumBits: 67108864\nspark.databricks.photon.shuffleRuntimeBloomFilterNumBits: 8388608\nspark.databricks.photon.shuffleRuntimeBloomFilterNumBitsPerRow: 8\nspark.databricks.photon.shuffleRuntimeBloomFilterNumPartitions: 10\nspark.databricks.photon.shuffleToBloomSizeRatio: 10\nspark.databricks.pyspark.pythonUdfsOnly: false\nspark.databricks.service.client.session.cache.size: 20\nspark.databricks.service.dbutils.fs.parallel.ls.threadPoolSize: 20\nspark.databricks.service.dbutils.fs.parallel.ls.timeoutSeconds: 7200\nspark.databricks.sql.externalUDF.perBatchTimeoutSeconds: 300\nspark.databricks.sql.externalUDF.sessionCreationTimeoutSeconds: 300\nspark.databricks.sql.files.prorateMaxPartitionBytes.ubound: 1073741824b\nspark.databricks.sql.functions.readFiles.optimizeLimit: true\nspark.databricks.sql.initial.catalog.name: hive_metastore\nspark.databricks.sql.maxTaskSizeForBucketedJoin: 268435456b\nspark.databricks.sql.minBucketsForBucketedJoin: <undefined>\nspark.databricks.sql.optimizer.maxAliasReplacementProjectListSize: 100\nspark.databricks.sql.optimizer.maxAliasReplacementProjectedExpressionsListSize: 500\nspark.databricks.streaming.schemaEvolution.enabled: true\nspark.databricks.thriftserver.getTables.threadPool.size: 64\nspark.databricks.thriftserver.metadata.timeout.seconds: 0ms\nspark.databricks.thriftserver.metadataTable.legacyMatch.enabled: false\nspark.databricks.unityCatalog.allowCrossMetastoreViews: false\nspark.databricks.unityCatalog.clientSideDualCatalog.enabled: true\nspark.databricks.unityCatalog.enabled: false\nspark.databricks.unityCatalog.thriftGetColumnsOptimization: true\nspark.databricks.unityCatalog.thriftGetColumnsOptimizationIgnoreUnnecessary: true\nspark.databricks.unityCatalog.validateDependencies: true\nspark.sql.adaptive.advisoryPartitionSizeInBytes: 67108864b\nspark.sql.adaptive.autoBroadcastJoinThreshold: <undefined>\nspark.sql.adaptive.coalescePartitions.enabled: true\nspark.sql.adaptive.coalescePartitions.initialPartitionNum: <undefined>\nspark.sql.adaptive.coalescePartitions.minPartitionSize: 1MB\nspark.sql.adaptive.coalescePartitions.parallelismFirst: true\nspark.sql.adaptive.customCostEvaluatorClass: <undefined>\nspark.sql.adaptive.enabled: true\nspark.sql.adaptive.forceOptimizeSkewedJoin: false\nspark.sql.adaptive.localShuffleReader.enabled: true\nspark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold: 0b\nspark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled: true\nspark.sql.adaptive.optimizer.excludedRules: <undefined>\nspark.sql.adaptive.rebalancePartitionsSmallPartitionFactor: 0.2\nspark.sql.adaptive.skewJoin.enabled: true\nspark.sql.adaptive.skewJoin.skewedPartitionFactor: 5.0\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes: 256MB\nspark.sql.ansi.doubleQuotedIdentifiers: false\nspark.sql.ansi.enabled: false\nspark.sql.ansi.enforceAnsiTypeCoercion: true\nspark.sql.ansi.enforceReservedKeywords: false\nspark.sql.ansi.relationPrecedence: false\nspark.sql.autoBroadcastJoinThreshold: 10MB\nspark.sql.avro.compression.codec: snappy\nspark.sql.avro.deflate.level: -1\nspark.sql.avro.filterPushdown.enabled: true\nspark.sql.broadcastTimeout: -1000ms\nspark.sql.bucketing.coalesceBucketsInJoin.enabled: false\nspark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio: 4\nspark.sql.cache.serializer: org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer\nspark.sql.catalog.spark_catalog: <undefined>\nspark.sql.catalog.spark_catalog.defaultDatabase: default\nspark.sql.cbo.enabled: true\nspark.sql.cbo.joinReorder.dp.star.filter: false\nspark.sql.cbo.joinReorder.dp.threshold: 12\nspark.sql.cbo.joinReorder.enabled: true\nspark.sql.cbo.planStats.enabled: false\nspark.sql.cbo.starSchemaDetection: false\nspark.sql.charAsVarchar: false\nspark.sql.cli.print.header: false\nspark.sql.columnNameOfCorruptRecord: _corrupt_record\nspark.sql.csv.filterPushdown.enabled: true\nspark.sql.datetime.java8API.enabled: false\nspark.sql.debug.maxToStringFields: 25\nspark.sql.defaultCatalog: spark_catalog\nspark.sql.error.messageFormat: PRETTY\nspark.sql.event.truncate.length: 2147483647\nspark.sql.execution.arrow.enabled: false\nspark.sql.execution.arrow.fallback.enabled: true\nspark.sql.execution.arrow.localRelationThreshold: 48MB\nspark.sql.execution.arrow.maxRecordsPerBatch: 10000\nspark.sql.execution.arrow.pyspark.enabled: false\nspark.sql.execution.arrow.pyspark.fallback.enabled: true\nspark.sql.execution.arrow.pyspark.selfDestruct.enabled: false\nspark.sql.execution.arrow.sparkr.enabled: false\nspark.sql.execution.pandas.udf.buffer.size: 65536\nspark.sql.execution.pyspark.udf.simplifiedTraceback.enabled: true\nspark.sql.execution.topKSortFallbackThreshold: 2147483632\nspark.sql.extensions: <undefined>\nspark.sql.files.ignoreCorruptFiles: false\nspark.sql.files.ignoreMissingFiles: false\nspark.sql.files.maxPartitionBytes: 128MB\nspark.sql.files.maxRecordsPerFile: 0\nspark.sql.files.minPartitionNum: <undefined>\nspark.sql.function.concatBinaryAsString: false\nspark.sql.function.eltOutputAsString: false\nspark.sql.groupByAliases: true\nspark.sql.groupByOrdinal: true\nspark.sql.hive.convertInsertingPartitionedTable: true\nspark.sql.hive.convertMetastoreCtas: true\nspark.sql.hive.convertMetastoreInsertDir: true\nspark.sql.hive.convertMetastoreOrc: true\nspark.sql.hive.convertMetastoreParquet: true\nspark.sql.hive.convertMetastoreParquet.mergeSchema: false\nspark.sql.hive.filesourcePartitionFileCacheSize: 262144000\nspark.sql.hive.manageFilesourcePartitions: true\nspark.sql.hive.metastore.barrierPrefixes: \nspark.sql.hive.metastore.jars: /databricks/databricks-hive/*\nspark.sql.hive.metastore.jars.path: \nspark.sql.hive.metastore.sharedPrefixes: org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\nspark.sql.hive.metastore.version: 0.13.0\nspark.sql.hive.metastorePartitionPruning: true\nspark.sql.hive.metastorePartitionPruningFallbackOnException: false\nspark.sql.hive.metastorePartitionPruningFastFallback: false\nspark.sql.hive.thriftServer.async: true\nspark.sql.hive.thriftServer.singleSession: false\nspark.sql.hive.verifyPartitionPath: false\nspark.sql.hive.version: 2.3.9\nspark.sql.inMemoryColumnarStorage.batchSize: 10000\nspark.sql.inMemoryColumnarStorage.compressed: true\nspark.sql.inMemoryColumnarStorage.enableVectorizedReader: true\nspark.sql.json.filterPushdown.enabled: true\nspark.sql.jsonGenerator.ignoreNullFields: true\nspark.sql.leafNodeDefaultParallelism: <undefined>\nspark.sql.mapKeyDedupPolicy: EXCEPTION\nspark.sql.maven.additionalRemoteRepositories: https://maven-central.storage-download.googleapis.com/maven2/\nspark.sql.maxMetadataStringLength: 100\nspark.sql.maxPlanStringLength: 2147483632\nspark.sql.maxSinglePartitionBytes: 9223372036854775807b\nspark.sql.metadataCacheTTLSeconds: -1000ms\nspark.sql.optimizer.collapseProjectAlwaysInline: false\nspark.sql.optimizer.dynamicPartitionPruning.enabled: true\nspark.sql.optimizer.enableCsvExpressionOptimization: true\nspark.sql.optimizer.enableJsonExpressionOptimization: true\nspark.sql.optimizer.excludedRules: <undefined>\nspark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold: 10GB\nspark.sql.optimizer.runtime.bloomFilter.creationSideThreshold: 10MB\nspark.sql.optimizer.runtime.bloomFilter.enabled: true\nspark.sql.optimizer.runtime.bloomFilter.expectedNumItems: 1000000\nspark.sql.optimizer.runtime.bloomFilter.maxNumBits: 67108864\nspark.sql.optimizer.runtime.bloomFilter.maxNumItems: 4000000\nspark.sql.optimizer.runtime.bloomFilter.numBits: 8388608\nspark.sql.optimizer.runtime.rowLevelOperationGroupFilter.enabled: true\nspark.sql.optimizer.runtimeFilter.number.threshold: 10\nspark.sql.orc.aggregatePushdown: false\nspark.sql.orc.columnarReaderBatchSize: 4096\nspark.sql.orc.compression.codec: snappy\nspark.sql.orc.enableNestedColumnVectorizedReader: true\nspark.sql.orc.enableVectorizedReader: true\nspark.sql.orc.filterPushdown: true\nspark.sql.orc.mergeSchema: false\nspark.sql.orderByOrdinal: true\nspark.sql.parquet.aggregatePushdown: false\nspark.sql.parquet.binaryAsString: false\nspark.sql.parquet.columnarReaderBatchSize: 4096\nspark.sql.parquet.compression.codec: snappy\nspark.sql.parquet.enableNestedColumnVectorizedReader: false\nspark.sql.parquet.enableVectorizedReader: true\nspark.sql.parquet.fieldId.read.enabled: true\nspark.sql.parquet.fieldId.read.ignoreMissing: false\nspark.sql.parquet.fieldId.write.enabled: true\nspark.sql.parquet.filterPushdown: true\nspark.sql.parquet.int96AsTimestamp: true\nspark.sql.parquet.int96TimestampConversion: false\nspark.sql.parquet.mergeSchema: false\nspark.sql.parquet.outputTimestampType: INT96\nspark.sql.parquet.recordLevelFilter.enabled: false\nspark.sql.parquet.respectSummaryFiles: false\nspark.sql.parquet.writeLegacyFormat: false\nspark.sql.parser.quotedRegexColumnNames: false\nspark.sql.pivotMaxValues: 10000\nspark.sql.pyspark.inferNestedDictAsStruct.enabled: false\nspark.sql.pyspark.jvmStacktrace.enabled: false\nspark.sql.queryExecutionListeners: <undefined>\nspark.sql.readSideCharPadding: false\nspark.sql.redaction.options.regex: (?i)url\nspark.sql.redaction.string.regex: <undefined>\nspark.sql.repl.eagerEval.enabled: false\nspark.sql.repl.eagerEval.maxNumRows: 20\nspark.sql.repl.eagerEval.truncate: 20\nspark.sql.session.timeZone: Etc/UTC\nspark.sql.shuffle.partitions: 200\nspark.sql.shuffledHashJoinFactor: 3\nspark.sql.sources.bucketing.autoBucketedScan.enabled: true\nspark.sql.sources.bucketing.enabled: true\nspark.sql.sources.bucketing.maxBuckets: 100000\nspark.sql.sources.default: delta\nspark.sql.sources.disabledJdbcConnProviderList: \nspark.sql.sources.parallelPartitionDiscovery.threshold: 32\nspark.sql.sources.partitionColumnTypeInference.enabled: true\nspark.sql.sources.partitionOverwriteMode: STATIC\nspark.sql.sources.v2.bucketing.enabled: false\nspark.sql.sources.v2.bucketing.pushPartValues.enabled: false\nspark.sql.statistics.fallBackToHdfs: false\nspark.sql.statistics.histogram.enabled: false\nspark.sql.statistics.size.autoUpdate.enabled: false\nspark.sql.storeAssignmentPolicy: ANSI\nspark.sql.streaming.checkpointLocation: <undefined>\nspark.sql.streaming.continuous.epochBacklogQueueSize: 10000\nspark.sql.streaming.disabledV2Writers: \nspark.sql.streaming.fileSource.cleaner.numThreads: 1\nspark.sql.streaming.forceDeleteTempCheckpointLocation: false\nspark.sql.streaming.metricsEnabled: false\nspark.sql.streaming.multipleWatermarkPolicy: min\nspark.sql.streaming.noDataMicroBatches.enabled: true\nspark.sql.streaming.numRecentProgressUpdates: 100\nspark.sql.streaming.sessionWindow.merge.sessions.in.local.partition: false\nspark.sql.streaming.stateStore.stateSchemaCheck: true\nspark.sql.streaming.stopActiveRunOnRestart: true\nspark.sql.streaming.stopTimeout: 15s\nspark.sql.streaming.streamingQueryListeners: <undefined>\nspark.sql.streaming.ui.enabled: true\nspark.sql.streaming.ui.retainedProgressUpdates: 100\nspark.sql.streaming.ui.retainedQueries: 100\nspark.sql.thriftServer.interruptOnCancel: false\nspark.sql.thriftServer.queryTimeout: 0ms\nspark.sql.thriftserver.scheduler.pool: <undefined>\nspark.sql.thriftserver.ui.retainedSessions: 200\nspark.sql.thriftserver.ui.retainedStatements: 200\nspark.sql.ui.explainMode: formatted\nspark.sql.ui.retainedExecutions: 1000\nspark.sql.variable.substitute: true\nspark.sql.warehouse.dir: dbfs:/user/hive/warehouse\nspark.thriftserver.arrowBasedRowSet.maxBytesPerFetchLimit: 10485760\nspark.thriftserver.arrowBasedRowSet.timestampAsString: true\nspark.thriftserver.cloudStoreBasedRowSet.driver.cloudUploadThreshold: 5242880b\nspark.thriftserver.cloudStoreBasedRowSet.enabled: true\nspark.thriftserver.cloudStoreBasedRowSet.executor.cloudUploadThreshold: 5242880b\nspark.thriftserver.cloudStoreBasedRowSet.maxBytesPerFetchLimit: 1073741824\nspark.thriftserver.cloudStoreBasedRowSet.resultFilesInMetadataLimit: 10000\nspark.thriftserver.cloudStoreBasedRowSet.smallResultsOptimization.enabled: true\nspark.thriftserver.cloudfetch.enabled: true\nspark.thriftserver.intervalsAsStringInResultSchema: true\nspark.thriftserver.operation.poll.timeout: 600000ms\nspark.thriftserver.operation.poll.timeout.enabled: true\nspark.thriftserver.resultCaching.eagerLookup.enabled: false\n"
     ]
    }
   ],
   "source": [
    "sql_configs = spark.sql(\"SET -v\").collect()\n",
    "for row in sql_configs:\n",
    "        print(f\"{row['key']}: {row['value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a38333-fba5-4391-9d94-762b7d4878bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "deploymode_setting = spark.sparkContext.getConf().get('spark.submit.deployMode') \n",
    "print(deploymode_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c441248-81d2-4ea5-9499-206e2170a78b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+---------+-----+\n| id|name|semester|  subject|marks|\n+---+----+--------+---------+-----+\n|  1|   A|       1|  PHYSICS|  100|\n|  1|   A|       2|  PHYSICS|  150|\n|  1|   A|       3|  PHYSICS|  200|\n|  1|   A|       4|  PHYSICS|  250|\n|  1|   A|       1|CHEMISTRY|   50|\n|  1|   A|       2|CHEMISTRY|  250|\n|  1|   A|       3|CHEMISTRY|  200|\n|  1|   A|       4|CHEMISTRY|  350|\n|  2|   B|       1|  PHYSICS|  150|\n|  2|   B|       2|  PHYSICS|  250|\n|  2|   B|       3|  PHYSICS|  100|\n|  2|   B|       4|  PHYSICS|  200|\n|  2|   B|       1|CHEMISTRY|  150|\n|  2|   B|       2|CHEMISTRY|  150|\n|  2|   B|       3|CHEMISTRY|  250|\n|  2|   B|       4|CHEMISTRY|  300|\n+---+----+--------+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (1,'A',1,'PHYSICS',100),\n",
    " (1,'A',2,'PHYSICS',150),\n",
    " (1,'A',3,'PHYSICS',200),\n",
    " (1,'A',4,'PHYSICS',250),\n",
    " (1,'A',1,'CHEMISTRY',50),\n",
    " (1,'A',2,'CHEMISTRY',250),\n",
    " (1,'A',3,'CHEMISTRY',200),\n",
    " (1,'A',4,'CHEMISTRY',350),\n",
    " (2,'B',1,'PHYSICS',150),\n",
    " (2,'B',2,'PHYSICS',250),\n",
    " (2,'B',3,'PHYSICS',100),\n",
    " (2,'B',4,'PHYSICS',200),\n",
    " (2,'B',1,'CHEMISTRY',150),\n",
    " (2,'B',2,'CHEMISTRY',150),\n",
    " (2,'B',3,'CHEMISTRY',250),\n",
    " (2,'B',4,'CHEMISTRY',300),\n",
    "]\n",
    "schema = ['id','name','semester','subject','marks']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6445f325-6eae-43b0-8cf4-66e692084692",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+\n|name|semester|marks|\n+----+--------+-----+\n|   B|       1|  300|\n|   A|       2|  400|\n|   B|       2|  400|\n|   A|       3|  400|\n|   A|       4|  600|\n+----+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "w = Window.partitionBy(\"name\", \"semester\")\n",
    "w1 = Window.partitionBy(\"semester\")\n",
    "df1 = df.withColumn(\"sum\", sum(\"marks\").over(w))\n",
    "df2 = df1.withColumn(\"max\", max(\"sum\").over(w1)).filter(col(\"sum\") == col(\"max\"))\n",
    "df3 = df2.select(\"name\", \"semester\", col(\"sum\").alias(\"marks\")).distinct()\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8502aef-a9d3-442c-bed8-8fd2ffbcf3f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date_col</th><th>last_day_of_month</th></tr></thead><tbody><tr><td>2023-02-18</td><td>2023-02-28</td></tr><tr><td>2024-02-25</td><td>2024-02-29</td></tr><tr><td>2019-03-01</td><td>2019-03-31</td></tr><tr><td>2024-11-28</td><td>2024-11-30</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2023-02-18",
         "2023-02-28"
        ],
        [
         "2024-02-25",
         "2024-02-29"
        ],
        [
         "2019-03-01",
         "2019-03-31"
        ],
        [
         "2024-11-28",
         "2024-11-30"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date_col",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_day_of_month",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [\n",
    " ('2023-02-18',),\n",
    " ('2024-02-25',),\n",
    " ('2019-03-01',),\n",
    " ('2024-11-28',)\n",
    "]\n",
    "\n",
    "schema = 'Date_col string'\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"last_day_of_month\", last_day(to_date(col(\"Date_col\"), \"yyyy-MM-dd\")))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53634cb4-c05b-41d8-8206-ea0dc8c00f8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "data1 = [(1,100),(2,200),(3,300)]\n",
    "schema1 =['ID','Value']\n",
    "df1 = spark.createDataFrame(data1,schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90fa04e-bb14-4bd1-809e-1721746baf35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+\n|Category|subCategory|Value|\n+--------+-----------+-----+\n|       A|          x|   10|\n|       A|          y|   20|\n|       B|          x|   30|\n|       B|          z|   40|\n+--------+-----------+-----+\n\n+-----------+---+---+\n|subCategory|  A|  B|\n+-----------+---+---+\n|          x| 10| 30|\n|          y| 20|  0|\n|          z|  0| 40|\n+-----------+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "data = [(\"A\",\"x\",10),\n",
    "        (\"A\",\"y\",20),\n",
    "        (\"B\",\"x\",30),\n",
    "        (\"B\",\"z\",40)]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"Category\",\"subCategory\",\"Value\")\n",
    "df.show()\n",
    "\n",
    "df1 = df.groupBy(\"subCategory\").pivot(\"Category\").sum(\"Value\").orderBy(\"subCategory\").fillna(0)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d194c8f6-f723-4d52-b8e9-ae1b52d9b6d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[34]: \"\\n%sql\\n-- Without Pivot keyword\\nselect SubCategory\\n , MAX( CASE WHEN Category = 'A' THEN Value ELSE 0 END) as 'A'\\n , MAX( CASE WHEN Category = 'B' THEN Value ELSE 0 END) as 'B'\\nfrom ( values \\n ('A','x',10)\\n ,('A','y',20)\\n ,('B','x',30)\\n ,('B','z',40)\\n )v (Category,SubCategory,Value)\\n group by SubCategory\\n-- With Pivot keyword\\nselect *\\nfrom ( values \\n ('A','x',10)\\n ,('A','y',20)\\n ,('B','x',30)\\n ,('B','z',40)\\n )v (Category,SubCategory,Value)\\n pivot(max(Value) for Category in (A,B))P\\n \""
     ]
    }
   ],
   "source": [
    "'''\n",
    "%sql\n",
    "-- Without Pivot keyword\n",
    "select SubCategory\n",
    " , MAX( CASE WHEN Category = 'A' THEN Value ELSE 0 END) as 'A'\n",
    " , MAX( CASE WHEN Category = 'B' THEN Value ELSE 0 END) as 'B'\n",
    "from ( values \n",
    " ('A','x',10)\n",
    " ,('A','y',20)\n",
    " ,('B','x',30)\n",
    " ,('B','z',40)\n",
    " )v (Category,SubCategory,Value)\n",
    " group by SubCategory\n",
    "-- With Pivot keyword\n",
    "select *\n",
    "from ( values \n",
    " ('A','x',10)\n",
    " ,('A','y',20)\n",
    " ,('B','x',30)\n",
    " ,('B','z',40)\n",
    " )v (Category,SubCategory,Value)\n",
    " pivot(max(Value) for Category in (A,B))P\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573367bf-6ef1-47da-9f98-880ee45075bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n|product_id|   store|price|\n+----------+--------+-----+\n|         1|    Shop|  110|\n|         1|LC_Store|  100|\n|         2|  Nozama|  200|\n|         2|    Souq|  190|\n|         3|    Shop| 1000|\n|         3|    Souq| 1900|\n+----------+--------+-----+\n\n+----------+--------+------+----+----+\n|product_id|LC_Store|Nozama|Shop|Souq|\n+----------+--------+------+----+----+\n|         1|     100|  null| 110|null|\n|         3|    null|  null|1000|1900|\n|         2|    null|   200|null| 190|\n+----------+--------+------+----+----+\n\n+----------+-----------+--------+------+----+----+\n|product_id|  unique_id|LC_Store|Nozama|Shop|Souq|\n+----------+-----------+--------+------+----+----+\n|         2|25769803776|    null|   200|null|null|\n|         3|51539607552|    null|  null|1000|null|\n|         1|17179869184|     100|  null|null|null|\n|         1| 8589934592|    null|  null| 110|null|\n|         2|42949672960|    null|  null|null| 190|\n|         3|60129542144|    null|  null|null|1900|\n+----------+-----------+--------+------+----+----+\n\n+----------+--------+-----+\n|product_id|   store|price|\n+----------+--------+-----+\n|         1|LC_Store|  100|\n|         1|    Shop|  110|\n|         3|    Shop| 1000|\n|         3|    Souq| 1900|\n|         2|  Nozama|  200|\n|         2|    Souq|  190|\n+----------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame\n",
    "data = [\n",
    "    (1, \"Shop\", 110),\n",
    "    (1, \"LC_Store\", 100),\n",
    "    (2, \"Nozama\", 200),\n",
    "    (2, \"Souq\", 190),\n",
    "    (3, \"Shop\", 1000),\n",
    "    (3, \"Souq\", 1900)\n",
    "]\n",
    "columns = [\"product_id\", \"store\", \"price\"]\n",
    "products_df = spark.createDataFrame(data, columns)\n",
    "products_df.show()\n",
    "\n",
    "# Pivot the DataFrame\n",
    "#pivot_df = products_df.groupBy(\"product_id\").pivot(\"store\").agg({\"price\": \"first\"})\n",
    "pivot_df = products_df.groupBy(\"product_id\").pivot(\"store\").agg(first(\"price\"))\n",
    "pivot_df.show()\n",
    "#OR\n",
    "# Add a unique identifier to each row\n",
    "products_df1 = products_df.withColumn(\"unique_id\", monotonically_increasing_id())\n",
    "# Pivot the DataFrame\n",
    "pivot_df1 = products_df1.groupBy(\"product_id\", \"unique_id\").pivot(\"store\").agg(first(\"price\"))\n",
    "pivot_df1.show()\n",
    "\n",
    "# Unpivot the DataFrame\n",
    "unpivot_expr = \"stack(4, 'LC_Store', LC_Store, 'Nozama', Nozama, 'Shop', Shop, 'Souq', Souq) as (store, price)\"\n",
    "unpivot_df = pivot_df.selectExpr(\"product_id\", unpivot_expr).where(\"price is not null\")\n",
    "unpivot_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77dd48a0-9a1d-4a9d-a618-4bcba82c3795",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+----------+----------+----------+----------+\n|company_id|        company_name|2024-01-01|2024-01-02|2024-01-03|2024-01-04|2024-01-05|\n+----------+--------------------+----------+----------+----------+----------+----------+\n|         1| Quantum Innovations|      12.5|      14.5|      16.5|      18.5|      20.5|\n|         2|   Stellar Solutions|      14.5|      16.5|      18.5|      20.5|      22.5|\n|         3|     Nebula Dynamics|      16.5|      18.5|      20.5|      22.5|      24.5|\n|         4|  Fusion Enterprises|      18.5|      20.5|      22.5|      24.5|      26.5|\n|         5|Celestial Technol...|      20.5|      22.5|      24.5|      26.5|      28.5|\n+----------+--------------------+----------+----------+----------+----------+----------+\n\n+-------------------+----------+-----------+\n|       company_name|      Date|Stock_price|\n+-------------------+----------+-----------+\n|Quantum Innovations|2024-01-01|       12.5|\n|Quantum Innovations|2024-01-02|       14.5|\n|Quantum Innovations|2024-01-03|       16.5|\n|Quantum Innovations|2024-01-04|       18.5|\n|Quantum Innovations|2024-01-05|       20.5|\n|  Stellar Solutions|2024-01-01|       14.5|\n|  Stellar Solutions|2024-01-02|       16.5|\n|  Stellar Solutions|2024-01-03|       18.5|\n|  Stellar Solutions|2024-01-04|       20.5|\n|  Stellar Solutions|2024-01-05|       22.5|\n|    Nebula Dynamics|2024-01-01|       16.5|\n|    Nebula Dynamics|2024-01-02|       18.5|\n|    Nebula Dynamics|2024-01-03|       20.5|\n|    Nebula Dynamics|2024-01-04|       22.5|\n|    Nebula Dynamics|2024-01-05|       24.5|\n| Fusion Enterprises|2024-01-01|       18.5|\n| Fusion Enterprises|2024-01-02|       20.5|\n| Fusion Enterprises|2024-01-03|       22.5|\n| Fusion Enterprises|2024-01-04|       24.5|\n| Fusion Enterprises|2024-01-05|       26.5|\n+-------------------+----------+-----------+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-768223720929573>:22\u001B[0m\n",
       "\u001B[1;32m     18\u001B[0m df1\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m#OR\u001B[39;00m\n",
       "\u001B[0;32m---> 22\u001B[0m df2 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcompany\u001B[39m\u001B[38;5;124m'\u001B[39m,expr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstack(5,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue1\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,value1,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue2\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,value2,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue3\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,value3,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue4\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,value4,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue5\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,value5) as (date,price)\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m     23\u001B[0m df2\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `company` cannot be resolved. Did you mean one of the following? [`company_id`, `company_name`, `2024-01-01`, `2024-01-02`, `2024-01-03`].;\n",
       "'Project ['company, 'stack(5, value1, 'value1, value2, 'value2, value3, 'value3, value4, 'value4, value5, 'value5) AS ArrayBuffer(date, price)]\n",
       "+- LogicalRDD [company_id#1939L, company_name#1940, 2024-01-01#1941, 2024-01-02#1942, 2024-01-03#1943, 2024-01-04#1944, 2024-01-05#1945], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-768223720929573>:22\u001B[0m\n\u001B[1;32m     18\u001B[0m df1\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m#OR\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m df2 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcompany\u001B[39m\u001B[38;5;124m'\u001B[39m,expr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstack(5,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue1\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,value1,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue2\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,value2,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue3\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,value3,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue4\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,value4,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue5\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,value5) as (date,price)\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     23\u001B[0m df2\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `company` cannot be resolved. Did you mean one of the following? [`company_id`, `company_name`, `2024-01-01`, `2024-01-02`, `2024-01-03`].;\n'Project ['company, 'stack(5, value1, 'value1, value2, 'value2, value3, 'value3, value4, 'value4, value5, 'value5) AS ArrayBuffer(date, price)]\n+- LogicalRDD [company_id#1939L, company_name#1940, 2024-01-01#1941, 2024-01-02#1942, 2024-01-03#1943, 2024-01-04#1944, 2024-01-05#1945], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `company` cannot be resolved. Did you mean one of the following? [`company_id`, `company_name`, `2024-01-01`, `2024-01-02`, `2024-01-03`].;\n'Project ['company, 'stack(5, value1, 'value1, value2, 'value2, value3, 'value3, value4, 'value4, value5, 'value5) AS ArrayBuffer(date, price)]\n+- LogicalRDD [company_id#1939L, company_name#1940, 2024-01-01#1941, 2024-01-02#1942, 2024-01-03#1943, 2024-01-04#1944, 2024-01-05#1945], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data for the DataFrame\n",
    "data = [\n",
    " (1, \"Quantum Innovations\", 12.5, 14.5, 16.5, 18.5, 20.5),\n",
    " (2, \"Stellar Solutions\", 14.5, 16.5, 18.5, 20.5, 22.5),\n",
    " (3, \"Nebula Dynamics\", 16.5, 18.5, 20.5, 22.5, 24.5),\n",
    " (4, \"Fusion Enterprises\", 18.5, 20.5, 22.5, 24.5, 26.5),\n",
    " (5, \"Celestial Technologies\", 20.5, 22.5, 24.5, 26.5, 28.5),\n",
    "]\n",
    "\n",
    "# Define the DataFrame\n",
    "df = spark.createDataFrame(data, [\"company_id\", \"company_name\", \"2024-01-01\", \"2024-01-02\", \"2024-01-03\", \"2024-01-04\", \"2024-01-05\"])\n",
    "df.show()\n",
    "\n",
    "#Databricks only has the unpivot option\n",
    "id_cols = [\"company_id\", \"company_name\"]\n",
    "value_columns = [ col_name   for col_name in df.columns   if  col_name not in id_cols ]\n",
    "df1 = df.unpivot( \"company_name\", value_columns , 'Date' , 'Stock_price' )\n",
    "df1.show()\n",
    "\n",
    "#OR\n",
    "\n",
    "df2 = df.select('company',expr(\"stack(5,'value1',value1,'value2',value2,'value3',value3,'value4',value4,'value5',value5) as (date,price)\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac5ee9f-39c1-4d98-be25-519a520e7fbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+\n| id|category|value1|value2|\n+---+--------+------+------+\n|  1|       A|    10|    20|\n|  2|       B|    30|    40|\n+---+--------+------+------+\n\n+---+--------+--------+-----+\n| id|category|variable|value|\n+---+--------+--------+-----+\n|  1|       A|  value1|   10|\n|  1|       A|  value2|   20|\n|  2|       B|  value1|   30|\n|  2|       B|  value2|   40|\n+---+--------+--------+-----+\n\n+---+--------+--------+-----+\n| id|category|variable|value|\n+---+--------+--------+-----+\n|  1|       A|  value1|   10|\n|  1|       A|  value2|   20|\n|  2|       B|  value1|   30|\n|  2|       B|  value2|   40|\n+---+--------+--------+-----+\n\nUnexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-2972675571587457>\", line 14, in <module>\n    unpivoted_df = df.withColumn(\"col\", array(lit(\"value1\"), lit(\"value2\"))) \\\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n    res = func(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/sql/dataframe.py\", line 2964, in __getattr__\n    raise AttributeError(\nAttributeError: 'DataFrame' object has no attribute 'explode'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'DataFrame' object has no attribute 'explode'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(1, \"A\", 10, 20), (2, \"B\", 30, 40)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"category\", \"value1\", \"value2\"])\n",
    "df.show()\n",
    "\n",
    "#Databricks only has the unpivot option\n",
    "unpivoted_df = df.unpivot(ids=[\"id\", \"category\"], values=[\"value1\", \"value2\"], variableColumnName=\"variable\", valueColumnName=\"value\")\n",
    "unpivoted_df.show()\n",
    "\n",
    "# Unpivot the DataFrame\n",
    "unpivot_expr = \"stack(2, 'value1', value1, 'value2', value2) as (variable, value)\"\n",
    "unpivoted_df = df.select(\"id\", \"category\", expr(unpivot_expr))\n",
    "unpivoted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6669868-f631-4d85-a07b-1cd7514c9a7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+\n| id|category|value1|value2|\n+---+--------+------+------+\n|  1|       A|    10|    20|\n|  2|       B|    30|    40|\n+---+--------+------+------+\n\n+---+--------+--------+-----+\n| id|category|variable|value|\n+---+--------+--------+-----+\n|  1|       A|  value1|   10|\n|  1|       A|  value2|   20|\n|  2|       B|  value1|   30|\n|  2|       B|  value2|   40|\n+---+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"A\", 10, 20), (2, \"B\", 30, 40)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"category\", \"value1\", \"value2\"])\n",
    "df.show()\n",
    "\n",
    "# Generate the stack expression dynamically\n",
    "num_columns = 2\n",
    "stack_expr_nw = \"stack({}, {}) as (variable, value)\".format(\n",
    "    num_columns,\n",
    "    \", \".join([f\"'value{i}', value{i}\" for i in range(1, num_columns + 1)])\n",
    ")\n",
    "\n",
    "# Unpivot the DataFrame\n",
    "unpivoted_df = df.selectExpr(\"id\", \"category\", stack_expr_nw)\n",
    "unpivoted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e3a45ef-4a8e-41db-8583-54f18c7d0b67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+\n| id|category|value1|value2|\n+---+--------+------+------+\n|  1|       A|    10|    20|\n|  2|       B|    30|    40|\n+---+--------+------+------+\n\n+---+--------+--------+\n| id|category|   value|\n+---+--------+--------+\n|  1|       A|[10, 20]|\n|  2|       B|[30, 40]|\n+---+--------+--------+\n\n+---+--------+---+--------+\n| id|category|col|variable|\n+---+--------+---+--------+\n|  1|       A| 10|  value1|\n|  1|       A| 20|  value2|\n|  2|       B| 30|  value1|\n|  2|       B| 40|  value2|\n+---+--------+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"A\", 10, 20), (2, \"B\", 30, 40)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"category\", \"value1\", \"value2\"])\n",
    "df.show()\n",
    "\n",
    "stage_df = df.withColumn(\"value\", array(col(\"value1\"), col(\"value2\"))) \\\n",
    "                .select(\"id\", \"category\",\"value\")\n",
    "stage_df.show()\n",
    "\n",
    "# Explode the array and create the variable column\n",
    "unpivoted_df = stage_df.select(\"id\", \"category\", posexplode(\"value\")) \\\n",
    "                        .withColumn(\"variable\", concat(lit(\"value\"), (col(\"pos\") + 1).cast(\"string\"))) \\\n",
    "                            .drop(\"pos\")\n",
    "\n",
    "# Show the result\n",
    "unpivoted_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f18f39d-5303-4c8f-bba9-fea3ded15dcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3527254897498342>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Unpivot the DataFrame\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m unpivoted_df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, array(lit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue1\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue2\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \\\n",
       "\u001B[1;32m      3\u001B[0m                  \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m, array(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue1\u001B[39m\u001B[38;5;124m\"\u001B[39m), col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue2\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \\\n",
       "\u001B[1;32m      4\u001B[0m                  \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m\"\u001B[39m, posexplode(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpos\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \\\n",
       "\u001B[1;32m      5\u001B[0m                  \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvariable\u001B[39m\u001B[38;5;124m\"\u001B[39m, col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m)[col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpos\u001B[39m\u001B[38;5;124m\"\u001B[39m)]) \\\n",
       "\u001B[1;32m      6\u001B[0m                  \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvariable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Show the result\u001B[39;00m\n",
       "\u001B[1;32m      9\u001B[0m unpivoted_df\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n",
       "\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n",
       "\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   4757\u001B[0m     )\n",
       "\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `col` cannot be resolved. Did you mean one of the following? [`pos`, `id`, `value`, `category`].;\n",
       "'Project [id#2929L, category#2930, pos#3026, value#3027L, 'col['pos] AS variable#3032]\n",
       "+- Project [id#2929L, category#2930, pos#3026, value#3027L]\n",
       "   +- Generate posexplode(value#3019), false, [pos#3026, value#3027L]\n",
       "      +- Project [id#2929L, category#2930, value1#2931L, value2#2932L, col#3013, array(value1#2931L, value2#2932L) AS value#3019]\n",
       "         +- Project [id#2929L, category#2930, value1#2931L, value2#2932L, array(value1, value2) AS col#3013]\n",
       "            +- LogicalRDD [id#2929L, category#2930, value1#2931L, value2#2932L], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3527254897498342>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Unpivot the DataFrame\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m unpivoted_df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, array(lit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue1\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue2\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \\\n\u001B[1;32m      3\u001B[0m                  \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m, array(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue1\u001B[39m\u001B[38;5;124m\"\u001B[39m), col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue2\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \\\n\u001B[1;32m      4\u001B[0m                  \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m\"\u001B[39m, posexplode(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpos\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \\\n\u001B[1;32m      5\u001B[0m                  \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvariable\u001B[39m\u001B[38;5;124m\"\u001B[39m, col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m)[col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpos\u001B[39m\u001B[38;5;124m\"\u001B[39m)]) \\\n\u001B[1;32m      6\u001B[0m                  \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvariable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Show the result\u001B[39;00m\n\u001B[1;32m      9\u001B[0m unpivoted_df\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   4757\u001B[0m     )\n\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `col` cannot be resolved. Did you mean one of the following? [`pos`, `id`, `value`, `category`].;\n'Project [id#2929L, category#2930, pos#3026, value#3027L, 'col['pos] AS variable#3032]\n+- Project [id#2929L, category#2930, pos#3026, value#3027L]\n   +- Generate posexplode(value#3019), false, [pos#3026, value#3027L]\n      +- Project [id#2929L, category#2930, value1#2931L, value2#2932L, col#3013, array(value1#2931L, value2#2932L) AS value#3019]\n         +- Project [id#2929L, category#2930, value1#2931L, value2#2932L, array(value1, value2) AS col#3013]\n            +- LogicalRDD [id#2929L, category#2930, value1#2931L, value2#2932L], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `col` cannot be resolved. Did you mean one of the following? [`pos`, `id`, `value`, `category`].;\n'Project [id#2929L, category#2930, pos#3026, value#3027L, 'col['pos] AS variable#3032]\n+- Project [id#2929L, category#2930, pos#3026, value#3027L]\n   +- Generate posexplode(value#3019), false, [pos#3026, value#3027L]\n      +- Project [id#2929L, category#2930, value1#2931L, value2#2932L, col#3013, array(value1#2931L, value2#2932L) AS value#3019]\n         +- Project [id#2929L, category#2930, value1#2931L, value2#2932L, array(value1, value2) AS col#3013]\n            +- LogicalRDD [id#2929L, category#2930, value1#2931L, value2#2932L], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "unpivoted_df = df.withColumn(\"col\", array(lit(\"value1\"), lit(\"value2\"))) \\\n",
    "                 .withColumn(\"value\", array(col(\"value1\"), col(\"value2\"))) \\\n",
    "                 .explode(\"col\") \\\n",
    "                 .select(\"id\", \"category\", \"col\", \"value\") \\\n",
    "                 .drop(\"col\")\n",
    "\n",
    "unpivoted_df.show()\n",
    "\n",
    "# Unpivot the DataFrame\n",
    "unpivoted_df = df.withColumn(\"col\", array(lit(\"value1\"), lit(\"value2\"))) \\\n",
    "                 .withColumn(\"value\", array(col(\"value1\"), col(\"value2\"))) \\\n",
    "                 .select(\"id\", \"category\", posexplode(col(\"value\")).alias(\"pos\", \"value\")) \\\n",
    "                 .withColumn(\"variable\", col(\"col\")[col(\"pos\")]) \\\n",
    "                 .select(\"id\", \"category\", \"variable\", \"value\")\n",
    "\n",
    "# Show the result\n",
    "unpivoted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62ef4faa-f741-490a-9321-2ec80fef57e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n|      Date|Value|\n+----------+-----+\n|2023-01-01|   10|\n|2023-01-02| null|\n|2023-01-03|   30|\n|2023-01-04| null|\n|2023-01-05|   50|\n+----------+-----+\n\n+----------+-----+------------------+\n|      Date|Value|fill_forward_value|\n+----------+-----+------------------+\n|2023-01-01|   10|                10|\n|2023-01-02| null|                10|\n|2023-01-03|   30|                30|\n|2023-01-04| null|                30|\n|2023-01-05|   50|                50|\n+----------+-----+------------------+\n\n+----------+-----+-----+---------------+\n|      Date|Value|Group|PreviousNotnull|\n+----------+-----+-----+---------------+\n|2023-01-01|   10|    1|             10|\n|2023-01-02| null|    1|             10|\n|2023-01-03|   30|    2|             30|\n|2023-01-04| null|    2|             30|\n|2023-01-05|   50|    3|             50|\n+----------+-----+-----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([StructField(\"Date\", StringType(), True), StructField(\"Value\", IntegerType(), True)])\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    " (\"2023-01-01\", 10),\n",
    " (\"2023-01-02\", None),\n",
    " (\"2023-01-03\", 30),\n",
    " (\"2023-01-04\", None),\n",
    " (\"2023-01-05\", 50)]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "window_forward = Window.partitionBy().orderBy('Date').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "df2=df.withColumn('fill_forward_value',last('Value',ignorenulls=True).over(window_forward))\n",
    "df2.show()\n",
    "\n",
    "df1 = df.select('*',sum(when(df.Value.isNull(),0).otherwise(1)).over(Window.orderBy(df.Date)).alias('Group') ) \n",
    "df1.select('*',first(df.Value).over(Window.partitionBy(df1.Group).orderBy(df1.Date)).alias('PreviousNotnull') ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6348e24-0403-47f8-bf6e-996d272aec5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|  category|brand_name|\n+----------+----------+\n|chocolates|    5-star|\n|      null|dairy milk|\n|      null|      perk|\n|      null|    eclair|\n|  Biscuits| britannia|\n|      null|  good day|\n|      null|     boost|\n+----------+----------+\n\n+----------+----------+\n|  category|brand_name|\n+----------+----------+\n|chocolates|    5-star|\n|chocolates|dairy milk|\n|chocolates|      perk|\n|chocolates|    eclair|\n|  Biscuits| britannia|\n|  Biscuits|  good day|\n|  Biscuits|     boost|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data =[('chocolates','5-star'),\n",
    "       (None,'dairy milk'),\n",
    "       (None,'perk'),\n",
    "       (None,'eclair'),\n",
    "       ('Biscuits','britannia'),\n",
    "       (None,'good day'),\n",
    "       (None,'boost')]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"category\" , \"brand_name\")\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"order_id\" , monotonically_increasing_id())\n",
    "w_forward = Window.partitionBy().orderBy(\"order_id\").rowsBetween(Window.unboundedPreceding,Window.currentRow) \n",
    "\n",
    "df.withColumn('category',last('category',ignorenulls=True).over(w_forward)).drop(\"order_id\").show()\n",
    "\n",
    "'''\n",
    "SELECT \n",
    " COALESCE(category, LAG(category IGNORE NULLS) OVER (ORDER BY monotonically_increasing_id())) AS category,\n",
    " brand_name\n",
    "FROM\n",
    " (SELECT *,\n",
    " ROW_NUMBER() OVER (ORDER BY monotonically_increasing_id()) AS order_id\n",
    " FROM my_table) t\n",
    "ORDER BY\n",
    " order_id;\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce61f642-dd27-4d08-83ed-5d8881ed6316",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|          name|\n+--------------+\n|         INDIA|\n|   AFGHANISTAN|\n|         CHINA|\n|     INDONESIA|\n|         NEPAL|\n|      PAKISTAN|\n|        RUSSIA|\n|     SINGAPORE|\n|     SRI LANKA|\n|UNITED KINGDOM|\n+--------------+\n\n+--------------+----+\n|          name|rank|\n+--------------+----+\n|         INDIA|   0|\n|   AFGHANISTAN|   1|\n|         CHINA|   2|\n|     INDONESIA|   4|\n|         NEPAL|   5|\n|      PAKISTAN|   6|\n|        RUSSIA|   7|\n|     SINGAPORE|   8|\n|     SRI LANKA|   9|\n|UNITED KINGDOM|  10|\n+--------------+----+\n\n+--------------+\n|          name|\n+--------------+\n|         INDIA|\n|   AFGHANISTAN|\n|         CHINA|\n|     INDONESIA|\n|         NEPAL|\n|      PAKISTAN|\n|        RUSSIA|\n|     SINGAPORE|\n|     SRI LANKA|\n|UNITED KINGDOM|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Given data\n",
    "data = [('INDONESIA',),('NEPAL',),('CHINA',),('PAKISTAN',),('SRI LANKA',),('INDIA',),('SINGAPORE',),('AFGHANISTAN',),('UNITED KINGDOM',),('RUSSIA',),]\n",
    "# Define schema\n",
    "schema = StructType([StructField('name', StringType(), True)])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "#1.\n",
    "sorted_df = df.orderBy(df['name'] != 'INDIA', df['name'])\n",
    "sorted_df.show()\n",
    "\n",
    "#2.\n",
    "Win = Window.orderBy(\"name\")\n",
    "df1 = df.withColumn(\"rank\", when(df.name == 'INDIA',0).otherwise(row_number().over(Win))).orderBy(\"rank\")\n",
    "df1.show()\n",
    "\n",
    "#3.\n",
    "df2 = df.filter(df[\"name\"] == \"INDIA\")\n",
    "df3 = df.filter(df[\"name\"] != \"INDIA\").orderBy(df2[\"name\"].asc())\n",
    "df4 = df2.union(df3)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb7b5ad-d3fb-416e-9453-68041066e123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n| name|WordLength|\n+-----+----------+\n| john|         4|\n|alice|         5|\n|  bob|         3|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "dfCount = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "dfCount.withColumn('WordLength',length('name')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bcbe4c9-3440-4483-a538-9f64da07bdc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"A\"),\n",
    "    (1, \"B\"),\n",
    "    (1, \"C\"),\n",
    "    (2, \"A\"),\n",
    "    (2, \"A\"),\n",
    "    (2, \"A\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "df.show()\n",
    "\n",
    "# Group by 'id' and collect list of 'name'\n",
    "df1 = df.groupBy(\"id\").agg(collect_list(\"name\").alias(\"names\"))\n",
    "df1.show()\n",
    "df3 = df1.withColumn(\"first_value\", col(\"names\").getItem(1))\n",
    "df3.show()\n",
    "\n",
    "df2 = df.groupBy(\"id\").agg(collect_set(\"name\").alias(\"names\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01956f7-e0d6-4875-b07b-b8ee18e55ab3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n|Column_1|Column_2|Column_3|\n+--------+--------+--------+\n|       A|      N1|      P1|\n|       A|      N2|      P2|\n|       A|      N3|      P3|\n|       B|      N1|      P1|\n|       C|      N1|      P1|\n|       C|      N2|      P2|\n+--------+--------+--------+\n\n+--------+--------+--------+\n|Column_1|Column_2|Column_3|\n+--------+--------+--------+\n|       A|N3,N2,N1|P1,P2,P3|\n|       B|      N1|      P1|\n|       C|   N2,N1|   P1,P2|\n+--------+--------+--------+\n\nOut[7]: \"\\nSELECT\\n column_1,\\n STRING_AGG(column_2, ', ') AS column_2,\\n STRING_AGG(column_3, ', ') AS column_3\\nFROM (\\n SELECT\\n column_1,\\n column_2,\\n column_3\\n FROM\\n Table\\n ORDER BY\\n column_2 DESC, \\n column_3 ASC \\n) AS OrderedData\\nGROUP BY\\n column_1;\\n \""
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (\"A\", \"N1\", \"P1\"),\n",
    " (\"A\", \"N2\", \"P2\"),\n",
    " (\"A\", \"N3\", \"P3\"),\n",
    " (\"B\", \"N1\", \"P1\"),\n",
    " (\"C\", \"N1\", \"P1\"),\n",
    " (\"C\", \"N2\", \"P2\")\n",
    "]\n",
    "\n",
    "schema = [\"Column_1\", \"Column_2\", \"Column_3\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "df1 = df.groupby(\"Column_1\") \\\n",
    "        .agg( array_join( reverse(collect_list( \"Column_2\" )), \",\" ).alias(\"Column_2\") \\\n",
    "                    , array_join( collect_list( \"Column_3\" ), \",\" ).alias(\"Column_3\")) \\\n",
    "        .sort(\"Column_1\") \n",
    "df1.show()\n",
    "'''\n",
    "SELECT\n",
    " column_1,\n",
    " STRING_AGG(column_2, ', ') AS column_2,\n",
    " STRING_AGG(column_3, ', ') AS column_3\n",
    "FROM (\n",
    " SELECT\n",
    " column_1,\n",
    " column_2,\n",
    " column_3\n",
    " FROM\n",
    " Table\n",
    " ORDER BY\n",
    " column_2 DESC, \n",
    " column_3 ASC \n",
    ") AS OrderedData\n",
    "GROUP BY\n",
    " column_1;\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b4ca0d3-96e2-4759-823e-5d1bdc7a6de7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Cathy\"),\n",
    "    (4, \"David\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "df.show()\n",
    "\n",
    "# Use collect to retrieve all rows as a list of Row objects\n",
    "collected_data = df.collect()\n",
    "\n",
    "# Print the collected data\n",
    "for row in collected_data:\n",
    "    print(f\"id: {row['id']}, name: {row['name']}\")\n",
    "\n",
    "# Get the first row\n",
    "first_row = collected_data[0]\n",
    "print(first_row)\n",
    "\n",
    "# Access the first column of the first row\n",
    "first_column_value = collected_data[0][0]\n",
    "print(first_column_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "033d3a44-7f5f-4d2f-99de-6733453843ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data with an array column\n",
    "data = [(1, [10, 20, 30]), (2, [40, 50, 60]), (3, [70, 80, 90])]\n",
    "df = spark.createDataFrame(data, [\"id\", \"values\"])\n",
    "df.show()\n",
    "\n",
    "# Extract the first element of the array in the 'values' column\n",
    "df_with_first_value = df.withColumn(\"first_value\", col(\"values\").getItem(0))\n",
    "df_with_first_value.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cab4c260-0732-4ee5-ad32-a74cdf6ca323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------------+\n|Student_id|Student_name|        Book_issued|\n+----------+------------+-------------------+\n|       101|        Mark|        White Tiger|\n|       102|         Ria|   The Fountainhead|\n|       102|         Ria|The Seceret History|\n|       101|        Mark|       Bhagwad Gita|\n|       103|         Loi|   The Fountainhead|\n+----------+------------+-------------------+\n\n+----------+------------+--------------------------------------+\n|Student_id|Student_name|Books                                 |\n+----------+------------+--------------------------------------+\n|101       |Mark        |White Tiger ; Bhagwad Gita            |\n|102       |Ria         |The Fountainhead ; The Seceret History|\n|103       |Loi         |The Fountainhead                      |\n+----------+------------+--------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "book_issued = [\n",
    " (101 ,'Mark' , \"White Tiger\"),\n",
    " (102 ,'Ria' , \"The Fountainhead\"),\n",
    " (102 ,'Ria' , \"The Seceret History\"),\n",
    " (101 ,'Mark' , \"Bhagwad Gita\"),\n",
    " (103 ,'Loi' , \"The Fountainhead\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(book_issued).toDF(\"Student_id\" , \"Student_name\" , \"Book_issued\")\n",
    "df.show()\n",
    "\n",
    "df1 = df.groupby(\"Student_id\", \"Student_name\") \\\n",
    "                .agg(concat_ws( \" ; \", collect_list(df.Book_issued)).alias(\"Books\"))\n",
    "df1.show(truncate=False)\n",
    "#select student_id, student_name, group_concat(books,';') as book_issued from books group by student_id,student_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a7209af-a614-4585-90fb-2877f275e594",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.withColumn('file_name',upper(split(split(input_file_name(),'/')[size(split(input_file_name(),'/'))-1],'\\.')[0]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7404c762-2490-443d-b8aa-9bb5591f13fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+\n|     cust_name| DOB|      name|\n+--------------+----+----------+\n|    roh05ith16|0516|    rohith|\n|  27Sa98ikrish|2798|  Saikrish|\n|     lax78mi43|7843|     laxmi|\n|      Gur23u98|2398|      Guru|\n|prath26yusha10|2610|prathyusha|\n+--------------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define the data\n",
    "data = [('roh05ith16',), ('27Sa98ikrish',), ('lax78mi43',), ('Gur23u98',), ('prath26yusha10',)]\n",
    "# Define the schema\n",
    "schema = [\"cust_name\"]\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "number_pattren = \"[^\\d]\" #or \"[^a-zA-Z]\" to extract numbers\n",
    "alpha_pattren = \"[\\d]\" #or [a-zA-Z] to extract alpha\n",
    "\n",
    "df = df.withColumn(\"DOB\", regexp_replace(col('cust_name'), number_pattren, \"\")) \\\n",
    "        .withColumn(\"name\", regexp_replace(col('cust_name'), alpha_pattren, \"\")) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d9d9df-eb53-4b9a-a969-61579bc3b1ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+--------+--------+\n|name|       phone|std_Code|landline|\n+----+------------+--------+--------+\n| ABC|040-20215632|     040|20215632|\n| XYZ|044-23651023|     044|23651023|\n| PQR| 086-1245678|     086| 1245678|\n+----+------------+--------+--------+\n\n+----+------------+--------+--------+\n|name|       phone|std_code|landline|\n+----+------------+--------+--------+\n| ABC|040-20215632|     040|20215632|\n| XYZ|044-23651023|     044|23651023|\n| PQR| 086-1245678|     086| 1245678|\n+----+------------+--------+--------+\n\n+----+------------+--------+--------+\n|name|       phone|std_code|landline|\n+----+------------+--------+--------+\n| ABC|040-20215632|     040|20215632|\n| XYZ|044-23651023|     044|23651023|\n| PQR| 086-1245678|     086| 1245678|\n+----+------------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    " StructField(\"name\", StringType(), True),\n",
    " StructField(\"phone\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create data\n",
    "data = [(\"ABC\", \"040-20215632\"),\n",
    "        (\"XYZ\", \"044-23651023\"),\n",
    "        (\"PQR\", \"086-1245678\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df_mod = df.withColumn(\"std_Code\", expr(\"substring(phone, 1, locate('-', phone) - 1)\")) \\\n",
    "            .withColumn(\"landline\", expr(\"substring(phone, locate('-', phone) + 1)\"))\n",
    "df_mod.show()\n",
    "\n",
    "df1 = df.withColumn(\"std_code\", split(col(\"phone\"),'-')[0]) \\\n",
    "        .withColumn(\"landline\", split(\"phone\",'-')[1])\n",
    "df1.show()\n",
    "\n",
    "df2 = df.withColumn('std_code', split(col('phone'),\"-\").getItem(0)) \\\n",
    "        .withColumn('landline', split(col('phone'),\"-\").getItem(1))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02cd17f3-f610-4984-9f6b-9e7032c66816",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n|device_id|   location|\n+---------+-----------+\n|       12|  bangalore|\n|       12|  bangalore|\n|       12|  bangalore|\n|       12|  bangalore|\n|       12|      hosur|\n|       12|      hosur|\n|       13|   hydrabad|\n|       13|   hydrabad|\n|       13|secundrabad|\n|       13|secundrabad|\n|       13|secundrabad|\n+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " Row(device_id=12, location='bangalore'),\n",
    " Row(device_id=12, location='bangalore'),\n",
    " Row(device_id=12, location='bangalore'),\n",
    " Row(device_id=12, location='bangalore'),\n",
    " Row(device_id=12, location='hosur'),\n",
    " Row(device_id=12, location='hosur'),\n",
    " Row(device_id=13, location='hydrabad'),\n",
    " Row(device_id=13, location='hydrabad'),\n",
    " Row(device_id=13, location='secundrabad'),\n",
    " Row(device_id=13, location='secundrabad'),\n",
    " Row(device_id=13, location='secundrabad'),\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the sample data and schema\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a053d188-a557-41cf-b0ff-a4625fa7e55c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n|device_id|total_signal|   location|\n+---------+------------+-----------+\n|       12|           6|  bangalore|\n|       13|           5|secundrabad|\n+---------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "w1 = Window.partitionBy(\"device_id\").orderBy(asc(\"device_id\"))\n",
    "w2 = Window.partitionBy(\"device_id\", \"location\").orderBy(asc(\"device_id\"))\n",
    "df1 = df.withColumn(\"rn\", row_number().over(w1)) \\\n",
    "            .withColumn(\"total_signal\", max(\"rn\").over(w1)) \\\n",
    "                .withColumn(\"rn1\", row_number().over(w2)) \\\n",
    "                    .withColumn(\"mx1\", max(\"rn1\").over(w1)) \\\n",
    "                            .filter(col(\"rn\") == col(\"mx1\")) \\\n",
    "                                .select(\"device_id\", \"total_signal\", \"location\")\n",
    "df1.show()                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4deb0e9-0307-4d74-b551-2219fd4a4b98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+\n|device_id|   location|total_count|\n+---------+-----------+-----------+\n|       12|  bangalore|          6|\n|       13|secundrabad|          5|\n+---------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spec1=Window.partitionBy(col(\"device_id\"))\n",
    "spec2=Window.partitionBy(col(\"location\"))\n",
    "\n",
    "dfmod = df.withColumn(\"total_count\",count(\"*\").over(spec1))\\\n",
    "            .withColumn(\"citywise_signal_count\",count(\"*\").over(spec2))\\\n",
    "                .withColumn(\"rank\",dense_rank().over(Window.partitionBy(col(\"device_id\")).orderBy(col(\"citywise_signal_count\").desc())))\n",
    "\n",
    "df2 = dfmod.filter(\"rank==1\").select(\"device_id\",\"location\",\"total_count\").distinct()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac4c8f2-f175-4429-9b3a-0ec73a3d3be4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+------+\n|emp_id| name|department|Salary|\n+------+-----+----------+------+\n|     1| Neha|        IT|  1000|\n|     2| John|        HR|  1500|\n|     3|David|        IT|  1500|\n+------+-----+----------+------+\n\n+------+-----+----------+------+\n|emp_id| name|department|Salary|\n+------+-----+----------+------+\n|     1| Neha|        IT|  1000|\n|     2| John|        HR|  1500|\n|     3|David|        IT|  1500|\n+------+-----+----------+------+\n\nOut[29]: DataFrame[emp_id: int, name: string, department: string, Salary: int]"
     ]
    }
   ],
   "source": [
    "emp_data = [\n",
    " (1,'Neha','IT',1000),\n",
    " (2,'John','HR',1500),\n",
    " (3,'David','IT',1500)\n",
    "]\n",
    "\n",
    "emp_schema = StructType([\n",
    " StructField(\"empID\",IntegerType()),\n",
    " StructField(\"c2\",StringType()),\n",
    " StructField(\"c3\",StringType()),\n",
    " StructField(\"Salary\",IntegerType())\n",
    "])\n",
    "\n",
    "emp_df = spark.createDataFrame(emp_data,emp_schema)\n",
    "\n",
    "result_df = emp_df.toDF(\"emp_id\",\"name\",\"department\",\"Salary\")\n",
    "result_df.show()\n",
    "\n",
    "#result_df1 = emp_df.withColumnsRenamed({\"empID\":\"emp_id\",\"c2\":\"name\",\"c3\":\"department\",\"Salary\":\"Salary\"})\n",
    "\n",
    "result_df2 = emp_df\\\n",
    ".withColumnRenamed(\"empID\",\"emp_id\")\\\n",
    ".withColumnRenamed(\"c2\",\"name\")\\\n",
    ".withColumnRenamed(\"c3\",\"department\")\n",
    "result_df2.show()\n",
    "\n",
    "emp_df.createOrReplaceTempView(\"emp\")\n",
    "spark.sql(\"\"\"SELECT empID AS emp_id,c2 AS name,c3 AS department,Salary FROM emp\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c59e8de-0338-433c-abac-099ddf67a4ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------+\n|Branch|Student|Maths_marks|\n+------+-------+-----------+\n| Delhi|   Neha|         90|\n+------+-------+-----------+\n\n+-------+-------+-------------+-----------+\n|Student| Branch|Science_marks|Maths_marks|\n+-------+-------+-------------+-----------+\n|   Arav|Kolkata|           79|         83|\n|   null|Kolkata|           89|         73|\n+-------+-------+-------------+-----------+\n\n+-------+-------+-----------+-------------+\n| Branch|Student|Maths_marks|Science_marks|\n+-------+-------+-----------+-------------+\n|  Delhi|   Neha|         90|         null|\n|Kolkata|   Arav|         83|           79|\n|Kolkata|   null|         73|           89|\n+-------+-------+-----------+-------------+\n\n+-------+-------+-----------+-------------+\n| Branch|Student|Maths_marks|Science_marks|\n+-------+-------+-----------+-------------+\n|  Delhi|   Neha|         90|        -9999|\n|Kolkata|   Arav|         83|           79|\n|Kolkata|unknown|         73|           89|\n+-------+-------+-----------+-------------+\n\nOut[33]: '\\nKey points:\\n-----------------\\ndtypes is a attribute available in Pyspark which is used to check the datatypes of columns in pyspark dataframe. Basically dtypes attribute returns a list of tuples, where each tuple contains the name of the column and its data type.\\nHere we are not using Union and UnionAll because the no of columns are not same of two dataframes.\\nUnionByName is used here just to tackle the column mismatch scenario. It has one property allowMissingColumns=True which handles the situation if any column is not present at the time of union it will place null values there instead of throwing the error.\\n'"
     ]
    }
   ],
   "source": [
    "# Define the schema for branch1 DataFrame\n",
    "schema_branch1 = StructType([\n",
    " StructField(\"Branch\", StringType(), True),\n",
    " StructField(\"Student\", StringType(), True),\n",
    " StructField(\"Maths_marks\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create branch1 DataFrame\n",
    "branch1_data = [(\"Delhi\", \"Neha\", 90)]\n",
    "branch1 = spark.createDataFrame(branch1_data, schema_branch1)\n",
    "branch1.show()\n",
    "\n",
    "# Define the schema for branch2 DataFrame\n",
    "schema_branch2 = StructType([\n",
    " StructField(\"Student\", StringType(), True),\n",
    "StructField(\"Branch\", StringType(), True),\n",
    " StructField(\"Science_marks\", IntegerType(), True),\n",
    " StructField(\"Maths_marks\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create branch2 DataFrame\n",
    "branch2_data = [\n",
    " (\"Arav\", \"Kolkata\", 79, 83),\n",
    " (None, \"Kolkata\", 89, 73)\n",
    "]\n",
    "branch2 = spark.createDataFrame(branch2_data, schema_branch2)\n",
    "branch2.show()\n",
    "\n",
    "merged_df=branch1.unionByName(branch2,allowMissingColumns=True)\n",
    "merged_df.show()\n",
    "\n",
    "for col_name, col_type in merged_df.dtypes:\n",
    "    # Replace null values based on data type\n",
    "    if col_type=='int':\n",
    "        #fillna method used to replace null values in pyspark dataframe\n",
    "        merged_df = merged_df.fillna({col_name: -9999})\n",
    "    elif col_type==\"string\":\n",
    "        merged_df = merged_df.fillna({col_name: \"unknown\"})\n",
    "merged_df.show()\n",
    "\n",
    "'''\n",
    "Key points:\n",
    "-----------------\n",
    "dtypes is a attribute available in Pyspark which is used to check the datatypes of columns in pyspark dataframe. Basically dtypes attribute returns a list of tuples, where each tuple contains the name of the column and its data type.\n",
    "Here we are not using Union and UnionAll because the no of columns are not same of two dataframes.\n",
    "UnionByName is used here just to tackle the column mismatch scenario. It has one property allowMissingColumns=True which handles the situation if any column is not present at the time of union it will place null values there instead of throwing the error.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bb447eb-b698-4e85-81a1-36dda875c08d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#take only date part from timestamp\n",
    "df1 = df.withColumn('Order_date', to_date(col('Order_date')).cast('date')) \n",
    "#OR\n",
    "df2 = df.withColumn('Order_date', split(col('order_date'), ' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "869c0b27-8a46-4d6f-a6ae-994208ee9066",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n|         item|count(bill_id)|\n+-------------+--------------+\n|     biriyani|             3|\n|         dosa|             1|\n|         idli|             2|\n|mineral water|             2|\n|         poha|             2|\n|         rice|             1|\n+-------------+--------------+\n\n+-------------+-----+\n|         item|count|\n+-------------+-----+\n|     biriyani|    3|\n|         dosa|    1|\n|         idli|    2|\n|mineral water|    2|\n|         poha|    2|\n|         rice|    1|\n+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (101, [\"dosa\", \"biriyani\", \"idli\"]),\n",
    " (102, [\"biriyani\", \"mineral water\"]),\n",
    " (103, [\"rice\", \"mineral water\", \"poha\"]),\n",
    " (109, [\"idli\", \"biriyani\", \"poha\"]),\n",
    "]\n",
    " \n",
    "schema = [\"bill_id\", \"food_items\"]\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df1 = df.withColumn('item',explode('food_items')) \\\n",
    "            .groupBy('item').agg(count('bill_id')) \\\n",
    "                .orderBy(asc('item'))\n",
    "df1.show()\n",
    "\n",
    "exploded_food_items_df = df.select(col(\"bill_id\"), explode(col(\"food_items\")).alias(\"item\"))\n",
    "group_food_items = exploded_food_items_df.groupBy(\"item\").count()\n",
    "group_food_items.orderBy(\"item\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae6f9a4-02cc-45c9-9f0f-2cfc34177a55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------------+--------------+---+------+\n|First Name|Last Name|              Type|    Department|YoE|Salary|\n+----------+---------+------------------+--------------+---+------+\n|     Aryan|    Singh|Full-time Employee|Administration|  2| 20000|\n|     Rohan|  Agarwal|            Intern|     Technical|  3|  5000|\n|      Riya|     Shah|Full-time Employee|Administration|  5| 10000|\n|      Yash|   Bhatia|Part-time Employee|     Technical|  7| 10000|\n|  Siddhant|   Khanna|Full-time Employee|    Management|  6| 20000|\n+----------+---------+------------------+--------------+---+------+\n\n+------------------+--------------+----------+---------+\n|              Type|Administration|Management|Technical|\n+------------------+--------------+----------+---------+\n|Full-time Employee|       15000.0|   20000.0|      0.0|\n|            Intern|           0.0|       0.0|   5000.0|\n|Part-time Employee|           0.0|       0.0|  10000.0|\n+------------------+--------------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'First Name': ['Aryan', 'Rohan', 'Riya', 'Yash', 'Siddhant'],\n",
    "        'Last Name': ['Singh', 'Agarwal', 'Shah', 'Bhatia', 'Khanna'],\n",
    "        'Type': ['Full-time Employee', 'Intern', 'Full-time Employee', 'Part-time Employee', 'Full-time Employee'],\n",
    "        'Department': ['Administration', 'Technical', 'Administration', 'Technical', 'Management'],\n",
    "        'YoE': [2, 3, 5, 7, 6],\n",
    "        'Salary': [20000, 5000, 10000, 10000, 20000]}\n",
    "\n",
    "pandas_df = pd.DataFrame(data) \n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "spark_df.show()\n",
    "spark_df.groupBy(\"Type\").pivot(\"Department\").agg( (avg(\"Salary\") )).na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cadf64de-a810-4d2d-be14-3fbf07404e7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+\n| Id|        Name|Salary|\n+---+------------+------+\n|  1|  Aditya Sen|   100|\n|  2|Bikramaditya|   200|\n|  3|      Mark T|   100|\n|  4|  D K aditya|   200|\n|  5|       Danny|   500|\n|  6|         Eli|   100|\n+---+------------+------+\n\n+---+------------+------+\n| Id|        Name|Salary|\n+---+------------+------+\n|  1|  Aditya Sen|   100|\n|  2|Bikramaditya|   200|\n|  4|  D K aditya|   200|\n+---+------------+------+\n\n+---+------------+------+\n| Id|        Name|Salary|\n+---+------------+------+\n|  1|  Aditya Sen|   100|\n|  2|Bikramaditya|   200|\n|  4|  D K aditya|   200|\n+---+------------+------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3400150458012990>:17\u001B[0m\n",
       "\u001B[1;32m     15\u001B[0m df2 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mName\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39milike(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%a\u001B[39;00m\u001B[38;5;124mditya\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m#OR\u001B[39;00m\n",
       "\u001B[0;32m---> 17\u001B[0m df3 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mName\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mrlike(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*aditya*\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m    917\u001B[0m     )\n",
       "\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n",
       "\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: Dangling meta character '*' near index 0\n",
       "*aditya*\n",
       "^"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-3400150458012990>:17\u001B[0m\n\u001B[1;32m     15\u001B[0m df2 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mName\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39milike(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%a\u001B[39;00m\u001B[38;5;124mditya\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m#OR\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m df3 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mName\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mrlike(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*aditya*\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m    917\u001B[0m     )\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Dangling meta character '*' near index 0\n*aditya*\n^",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: Dangling meta character '*' near index 0\n*aditya*\n^",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    " (1, \"Aditya Sen\" , 100),\n",
    " (2, \"Bikramaditya\" , 200),\n",
    " (3, \"Mark T\" , 100),\n",
    " (4, \"D K aditya\" , 200), \n",
    " (5, \"Danny\" , 500),\n",
    " (6, \"Eli\" , 100),\n",
    "]\n",
    "df = spark.createDataFrame(data).toDF(\"Id\" ,\"Name\" , \"Salary\")\n",
    "df.show()\n",
    "\n",
    "# The (?i) at the beginning makes the pattern case-insensitive, and \"aditya\" is the pattern to match.\n",
    "df1 = df.filter(col(\"Name\").rlike(\"(?i)aditya\")).show()\n",
    "#OR\n",
    "df2 = df.filter(col(\"Name\").ilike(\"%aditya%\")).show()\n",
    "\n",
    "#select * from df where Name ~* '.*aditya.*';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c57b9767-2c97-4d48-8344-6e21bad25aff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    " .format(\"csv\") \\\n",
    " .option(\"header\", \"true\") \\\n",
    " .option(\"inferschema\", \"true\") \\\n",
    " .load(\"data/emp_data.csv\")\n",
    "'''\n",
    "CSV data\n",
    "======\n",
    "Id,first,last,age,salary\n",
    "1,Neha,D,#,100\n",
    "2,#,K,,200\n",
    "3,$,B,30,200\n",
    "4,NA,M,35,300\n",
    "5,Mark,L,29,100\n",
    "6,Ron,S,$,500\n",
    "'''\n",
    "# Specify multiple null values\n",
    "null_values = [\"$\", \"#\", \"&\", 'NA']\n",
    "\n",
    "# Replace specified null values with None\n",
    "for col in df.columns:\n",
    "    df = df.withColumn(col, when(df[col].isin(null_values), None).otherwise(df[col]))\n",
    "df.show()\n",
    "\n",
    "#OR\n",
    "df2 = df.withColumn(\n",
    " \"unexpectedResults\",\n",
    " when(reduce(lambda x, y: x | y, (col(c).isin(null_values) for c in df.columns)), False).otherwise(True)\n",
    ")\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b95884c8-1015-459c-83a8-08c09f04e741",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  4|\n|  5|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (1, ),\n",
    " (2,),\n",
    " (3,),\n",
    " (6,),\n",
    " (7,),\n",
    " (8,)]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"id\")\n",
    "\n",
    "min_value = df.select(min(df[\"id\"])).first()[0] #or df.agg(min(\"id\")).collect()[0][0]\n",
    "max_value = df.select(max(df[\"id\"])).first()[0] #or df.agg(max(\"id\")).collect()[0][0]+1\n",
    "df_with_all_numbers = spark.range(min_value, max_value+1).toDF(\"id\")\n",
    "df_missing_numbers = df_with_all_numbers.join(df,on=\"id\",how=\"left_anti\")\n",
    "df_missing_numbers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "829c8e87-7883-40a1-bada-b68a0674e063",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|student|\n+---+-------+\n|  1|  Abbot|\n|  2|  Doris|\n|  3|Emerson|\n|  4|  Green|\n|  5| Jeames|\n+---+-------+\n\n+---+-------+\n| id|student|\n+---+-------+\n|  1|  Doris|\n|  2|  Abbot|\n|  3|  Green|\n|  4|Emerson|\n|  5| Jeames|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "seat_data = [\n",
    " (1 , 'Abbot'),\n",
    " (2, 'Doris'),\n",
    " (3, 'Emerson' ),\n",
    " (4, 'Green'),\n",
    " (5,'Jeames' )\n",
    "]\n",
    " \n",
    "seat_schema = StructType(\n",
    "[\n",
    " StructField(\"id\" , IntegerType()),\n",
    " StructField(\"student\" , StringType())\n",
    "])\n",
    "\n",
    "seat_df = spark.createDataFrame(seat_data ,seat_schema )\n",
    "seat_df.show()\n",
    "\n",
    "df = seat_df.withColumn(\"previous_student\" , lag(\"student\").over(Window.orderBy(\"id\")) ) \\\n",
    "            .withColumn(\"next_student\" , lead(\"student\").over(Window.orderBy(\"id\")) )\n",
    "\n",
    "df1 = df.withColumn(\"exchange\" ,coalesce( when( col(\"id\")%2==0 ,df.previous_student )\n",
    "                                            .when( col(\"id\")%2!=0 ,df.next_student) , df.student)) \\\n",
    "        .select(\"id\", \"exchange\") \\\n",
    "        .withColumnRenamed(\"exchange\", \"student\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0afbbf66-f002-4d4c-b7a6-d8bb43cc336f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3400150458012995>:18\u001B[0m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Create DataFrame\u001B[39;00m\n",
       "\u001B[1;32m     17\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, schema)\n",
       "\u001B[0;32m---> 18\u001B[0m masked_card_number \u001B[38;5;241m=\u001B[39m concat(substring(df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcard_number\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m4\u001B[39m), \n",
       "\u001B[1;32m     19\u001B[0m                             lit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mlen\u001B[39m(df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcard_number\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m8\u001B[39m)), \n",
       "\u001B[1;32m     20\u001B[0m                             substring(df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcard_number\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m4\u001B[39m))\n",
       "\u001B[1;32m     21\u001B[0m masked_card_number\u001B[38;5;241m.\u001B[39mshow()      \n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m#OR\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2918\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n",
       "\u001B[1;32m   2876\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001B[39;00m\n",
       "\u001B[1;32m   2877\u001B[0m \n",
       "\u001B[1;32m   2878\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2915\u001B[0m \u001B[38;5;124;03m+---+----+\u001B[39;00m\n",
       "\u001B[1;32m   2916\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   2917\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m-> 2918\u001B[0m     jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\u001B[1;32m   2920\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Column):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `card_number` cannot be resolved. Did you mean one of the following? [`id`, `name`, `credit_card_no`]."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3400150458012995>:18\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Create DataFrame\u001B[39;00m\n\u001B[1;32m     17\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, schema)\n\u001B[0;32m---> 18\u001B[0m masked_card_number \u001B[38;5;241m=\u001B[39m concat(substring(df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcard_number\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m4\u001B[39m), \n\u001B[1;32m     19\u001B[0m                             lit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mlen\u001B[39m(df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcard_number\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m8\u001B[39m)), \n\u001B[1;32m     20\u001B[0m                             substring(df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcard_number\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m4\u001B[39m))\n\u001B[1;32m     21\u001B[0m masked_card_number\u001B[38;5;241m.\u001B[39mshow()      \n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m#OR\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2918\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2876\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001B[39;00m\n\u001B[1;32m   2877\u001B[0m \n\u001B[1;32m   2878\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2915\u001B[0m \u001B[38;5;124;03m+---+----+\u001B[39;00m\n\u001B[1;32m   2916\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2917\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m-> 2918\u001B[0m     jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n\u001B[1;32m   2920\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Column):\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `card_number` cannot be resolved. Did you mean one of the following? [`id`, `name`, `credit_card_no`].",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `card_number` cannot be resolved. Did you mean one of the following? [`id`, `name`, `credit_card_no`].",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    " StructField(\"id\", IntegerType(), True),\n",
    " StructField(\"name\", StringType(), True),\n",
    " StructField(\"credit_card_no\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    " (1, \"Rahul\", \"1234567891234567\"),\n",
    " (2, \"Raj\", \"1004567892345678\"),\n",
    " (3, \"Priya\", \"0234567893456789\"),\n",
    " (4, \"Murti\", \"2234567890123456\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "masked_card_number = concat(substring(df[\"card_number\"], 1, 4), \n",
    "                            lit(\"*\" * (len(df[\"card_number\"]) - 8)), \n",
    "                            substring(df[\"card_number\"], -4, 4))\n",
    "masked_card_number.show()      \n",
    "\n",
    "#OR\n",
    "\n",
    "def mask_credit_card(card_number):\n",
    "    if isinstance(card_number, str) and len(card_number) == 16:\n",
    "        return card_number[:4] + '*' * 8 + card_number[-4:]\n",
    "    else:\n",
    "        raise ValueError(\"Credit card number must be a 16-digit string.\")\n",
    "\n",
    "masked_data = [(id, name, mask_credit_card(credit_card)) for id, name, credit_card in data]\n",
    "masked_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3501e762-4a8a-487d-a1c6-41ed7d13114f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+------+----------+\n| Id| Name| Age|Salary|Department|\n+---+-----+----+------+----------+\n|  1| Neha|  30|  null|        IT|\n|  2| Mark|null|  null|        HR|\n|  3|David|  25|  null|        HR|\n|  4|Carol|  30|  null|      null|\n+---+-----+----+------+----------+\n\n['Age', 'Salary', 'Department']\n['Id', 'Name']\n['Salary']\n"
     ]
    }
   ],
   "source": [
    "emp_data = [\n",
    " (1,'Neha' , 30 , None, 'IT'),\n",
    " (2,'Mark' , None , None, 'HR'),\n",
    " (3,'David' , 25 , None, 'HR'),\n",
    " (4,'Carol' , 30 , None, None)\n",
    "]\n",
    "\n",
    "emp_schema = StructType([\n",
    " StructField(\"Id\" , IntegerType()) ,\n",
    " StructField(\"Name\" , StringType()) ,\n",
    " StructField(\"Age\" , IntegerType()) ,\n",
    " StructField(\"Salary\" , IntegerType()) ,\n",
    " StructField(\"Department\" , StringType()) ]\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(data = emp_data , schema =emp_schema )\n",
    "df.show()\n",
    "\n",
    "any_null_value_columns = [ col_name  for col_name in df.columns  if (df.filter( isnull(col(col_name)))).count()!=0]\n",
    "print(any_null_value_columns)\n",
    "\n",
    "no_null_value_columns = [ col_name  for col_name in df.columns  if (df.filter( isnull(col(col_name)))).count()==0]\n",
    "print(no_null_value_columns)\n",
    "\n",
    "all_null_value_columns = [ col_name  for col_name in df.columns  if (df.filter( (col(col_name)).isNotNull())).count()==0]\n",
    "print(all_null_value_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f5c9e1-3f62-4bf6-ab39-e800eb5e2238",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n|customer_name                  |\n+-------------------------------+\n|James                          |\n|Emily Davis                    |\n|Sophia Grace Lee               |\n|Alexander William Johnson Smith|\n+-------------------------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/column.py:466: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------------------------------------+----+----------+---------------+---------+\n|customer_name                  |list_of_words_in_name               |size|first_name|middle_name    |last_name|\n+-------------------------------+------------------------------------+----+----------+---------------+---------+\n|James                          |[James]                             |1   |James     |null           |null     |\n|Emily Davis                    |[Emily, Davis]                      |2   |Emily     |null           |Davis    |\n|Sophia Grace Lee               |[Sophia, Grace, Lee]                |3   |Sophia    |Grace          |Lee      |\n|Alexander William Johnson Smith|[Alexander, William, Johnson, Smith]|4   |Alexander |William Johnson|Smith    |\n+-------------------------------+------------------------------------+----+----------+---------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for the DataFrame\n",
    "schema = StructType([StructField(\"customer_name\", StringType(), True)])\n",
    "\n",
    "# Sample customer names data\n",
    "data = [(\"James\",),      # One word\n",
    " (\"Emily Davis\",),   # Two words\n",
    " (\"Sophia Grace Lee\",),# Three words\n",
    " (\"Alexander William Johnson Smith\",)] # Four words\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate =False)\n",
    "'''\n",
    "#split the name into a list of words and save in a new column , e.g [Sophia, Grace, Lee]\n",
    "# create a size column, if customer_name has three words, size=3\n",
    "# create 'first_name' column, first word of the list will be first_name , e.g.Sophia\n",
    "# if customer name has more than two words, then all words except first and last word will be under middle_name . e.g.Grace\n",
    "# slice the list_of_words_in_name , start -2 , length = length of list -2\n",
    "# last_name column will have the last word of customer name word list , e.g. Lee\n",
    "'''\n",
    "df =(\n",
    " df.withColumn(\"list_of_words_in_name\" , split(df['customer_name'] , ' ') ) \n",
    "    .withColumn(\"size\" , size(split(df['customer_name'] , ' ') )) \n",
    "    .withColumn(\"first_name\" , col('list_of_words_in_name').getItem(0)) \n",
    "    .withColumn(\"middle_name\",  when(col('size')>2 ,concat_ws(' ', slice(\"list_of_words_in_name\", lit(2), col(\"size\")-2))).otherwise(None))\n",
    "    .withColumn(\"last_name\" , when(col('size')>1 , col('list_of_words_in_name').getItem(col('size')-1)).otherwise(None)) \n",
    ")\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b14da97d-c390-4b7f-a2cd-4ce282520bc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n|      date|revenue|\n+----------+-------+\n|2024-01-01| 1000.5|\n|2024-01-02|1500.75|\n|2024-02-03|2000.25|\n|2024-03-04| 1800.0|\n|2024-02-05| 2200.5|\n|2024-01-03| 1000.5|\n|2024-01-04|1500.75|\n|2024-02-04|2000.25|\n|2024-03-05| 1800.0|\n|2024-02-06| 2200.5|\n+----------+-------+\n\nroot\n |-- date: date (nullable = true)\n |-- revenue: double (nullable = true)\n\n+----------+-------+------------------+\n|      date|revenue|cumulative_revenue|\n+----------+-------+------------------+\n|2024-01-01| 1000.5|            1000.5|\n|2024-01-02|1500.75|           2501.25|\n|2024-01-03| 1000.5|           3501.75|\n|2024-01-04|1500.75|            5002.5|\n|2024-02-03|2000.25|           2000.25|\n|2024-02-04|2000.25|            4000.5|\n|2024-02-05| 2200.5|            6201.0|\n|2024-02-06| 2200.5|            8401.5|\n|2024-03-04| 1800.0|            1800.0|\n|2024-03-05| 1800.0|            3600.0|\n+----------+-------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (\"2024-01-01\", 1000.50),\n",
    " (\"2024-01-02\", 1500.75),\n",
    " (\"2024-02-03\", 2000.25),\n",
    " (\"2024-03-04\", 1800.00),\n",
    " (\"2024-02-05\", 2200.50),\n",
    " (\"2024-01-03\", 1000.50),\n",
    " (\"2024-01-04\", 1500.75),\n",
    " (\"2024-02-04\", 2000.25),\n",
    " (\"2024-03-05\", 1800.00),\n",
    " (\"2024-02-06\", 2200.50)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    " StructField(\"date\", StringType(), True),\n",
    " StructField(\"revenue\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df=df.withColumn(\"date\" , to_date('date'))\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "window_spec = Window.partitionBy(month(\"date\")).orderBy(\"date\").rowsBetween(Window.unboundedPreceding , Window.currentRow)\n",
    "df = df.withColumn(\"cumulative_revenue\" , sum(\"revenue\").over(window_spec) )\n",
    "df.sort(\"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe63648-d5da-4211-af02-f0006ea0620b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3034563533578095>:11\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m col_mapping \u001B[38;5;241m=\u001B[39m{}\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col_name \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns:\n",
       "\u001B[1;32m     10\u001B[0m  \u001B[38;5;66;03m# FirstName , spilt when capital letter found , join the split word list with '_'\u001B[39;00m\n",
       "\u001B[0;32m---> 11\u001B[0m     new_col_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([s \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m([A-Z][^A-Z]*)\u001B[39m\u001B[38;5;124m\"\u001B[39m, col_name) \u001B[38;5;28;01mif\u001B[39;00m s])\u001B[38;5;241m.\u001B[39mlower() \n",
       "\u001B[1;32m     12\u001B[0m     col_mapping[col_name] \u001B[38;5;241m=\u001B[39m new_col_name\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m column_name \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns:\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 're' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3034563533578095>:11\u001B[0m\n\u001B[1;32m      8\u001B[0m col_mapping \u001B[38;5;241m=\u001B[39m{}\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col_name \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[1;32m     10\u001B[0m  \u001B[38;5;66;03m# FirstName , spilt when capital letter found , join the split word list with '_'\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     new_col_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([s \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m([A-Z][^A-Z]*)\u001B[39m\u001B[38;5;124m\"\u001B[39m, col_name) \u001B[38;5;28;01mif\u001B[39;00m s])\u001B[38;5;241m.\u001B[39mlower() \n\u001B[1;32m     12\u001B[0m     col_mapping[col_name] \u001B[38;5;241m=\u001B[39m new_col_name\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m column_name \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns:\n\n\u001B[0;31mNameError\u001B[0m: name 're' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 're' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    " (101 , 'Neha' , 'K'),\n",
    " (102 , 'Mark' , 'T'),\n",
    " (103 , 'Iram' , 'D')\n",
    "]\n",
    "df = spark.createDataFrame(data , ['Id' , 'FirstName' , 'LastName'])\n",
    "\n",
    "col_mapping ={}\n",
    "for col_name in df.columns:\n",
    " # FirstName , spilt when capital letter found , join the split word list with '_'\n",
    "    new_col_name = '_'.join([s for s in re.split(\"([A-Z][^A-Z]*)\", col_name) if s]).lower() \n",
    "    col_mapping[col_name] = new_col_name\n",
    " \n",
    "\n",
    "for column_name in df.columns:\n",
    "    df= df.withColumnRenamed(column_name, col_mapping[column_name])\n",
    "df.show()\n",
    "\n",
    "\n",
    "####################################\n",
    "\n",
    "def camel_to_snake_column_names(df):\n",
    "    for column in df.columns:\n",
    "        new_column_name = ''.join(['_' + c.lower() if c.isupper() and i != 0 else c.lower() for i, c in enumerate(column)])\n",
    "        df = df.withColumnRenamed(column, new_column_name)\n",
    "    return df\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df = camel_to_snake_column_names(df)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415b3249-61ef-4299-b777-a5530fb83fad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------+----------+\n|ProductCode|Quantity|UnitPrice|CustomerID|\n+-----------+--------+---------+----------+\n|       P001|       5|     20.0|      C001|\n|       P002|       3|     15.5|      C002|\n|       P003|      10|     5.99|      C003|\n|       P004|       2|     50.0|      C001|\n|       P005|   eight|    12.75|      C002|\n+-----------+--------+---------+----------+\n\n+-----------+--------+---------+----------+\n|ProductCode|Quantity|UnitPrice|CustomerID|\n+-----------+--------+---------+----------+\n|       P005|   eight|    12.75|      C002|\n+-----------+--------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    " StructField(\"ProductCode\", StringType(), True),\n",
    " StructField(\"Quantity\", StringType(), True),\n",
    " StructField(\"UnitPrice\", StringType(), True),\n",
    " StructField(\"CustomerID\", StringType(), True),\n",
    "])\n",
    " \n",
    "data = [\n",
    " (\"P001\", 5, 20.0, \"C001\"),\n",
    " (\"P002\", 3, 15.5, \"C002\"),\n",
    " (\"P003\", 10, 5.99, \"C003\"),\n",
    " (\"P004\", 2, 50.0, \"C001\"),\n",
    " (\"P005\", \"eight\", 12.75, \"C002\"),\n",
    "]\n",
    " \n",
    "product_df = spark.createDataFrame(data, schema=schema)\n",
    "product_df.show()\n",
    "product_df.filter(~col(\"Quantity\").rlike(\"^[0-9]*$\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cdf701c-47c2-4b84-9a36-f101c77b09ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "MOST_IMPORTANT",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
